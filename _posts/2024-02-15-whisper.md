---
layout:     post
title:      "Robust Speech Recognition via Large-Scale Weak Supervision论文解读" 
author:     "lili" 
mathjax: true
sticky: false
excerpt_separator: <!--more-->
tags:
    - Pre-training
    - Speech
    - ASR
---

本文是论文[Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356)的解读。

<!--more-->

**目录**
* TOC
{:toc}


## Abstract
 
我们研究了简单训练以预测互联网音频大量转录的语音处理系统的能力。当扩展到 68 万小时的多语言和多任务监督时，所得到的模型在标准基准上具有良好的泛化能力，并且通常与先前完全监督的结果竞争力相当，但在零样本迁移设置下无需任何微调。与人类相比，这些模型接近它们的准确性和鲁棒性。我们发布了模型和推断代码，作为进一步研究鲁棒语音处理的基础。







## 1 Introduction

在语音识别方面取得的进展受到了无监督预训练技术的推动，Wav2Vec 2.0（Baevski等人，2020）是其中的代表。由于这些方法直接从原始音频学习而无需人类标签，它们可以有效地利用大量未标记的语音数据集，并且已经迅速扩展到了100万小时的训练数据（Zhang等人，2021），远远超过了学术监督数据集通常的1000小时左右。当在标准基准上进行微调时，这种方法改进了现有技术水平，特别是在低数据情况下。

这些预训练的音频编码器学习了高质量的语音表示，但由于它们纯粹是无监督的，因此它们缺乏一个同样高性能的解码器，将这些表示映射到可用的输出，从而需要一个微调阶段来实际执行诸如语音识别之类的任务。不幸的是，这限制了它们的实用性和影响，因为微调仍然可能是一个需要熟练从业者的复杂过程。需要微调也存在额外的风险。机器学习方法非常擅长在训练数据集中找到模式，从而提高在相同数据集中的保留数据上的性能。然而，其中一些模式是脆弱的和虚假的，并且不适用于其他数据集和分布。在一个特别令人不安的例子中，Radford等人（2021）在未观察到在对七个其他自然图像数据集上分类相同对象时的平均准确率改善的情况下，在对ImageNet数据集（Russakovsky等人，2015）进行微调时，记录了物体分类准确率增加了9.2%。当在一个数据集上训练的模型在另一个数据集上评估时仍然可能出现许多基本错误，可能正是因为它正在利用那些人类忽视的特定于数据集的怪癖。

这表明，虽然无监督的预训练大大提高了音频编码器的质量，但由于缺乏同等高质量的预训练解码器，再加上建议的数据集特定微调协议，这是一个重要的弱点，限制了它们的实用性和鲁棒性。语音识别系统的目标应该是在广泛的环境中可靠地“开箱即用”，而不需要针对每个部署分布进行解码器的监督微调。

正如Narayanan等人（2018）、Likhomanenko等人（2020）和Chan等人（2021）所示，以监督方式在许多数据集/领域上预训练的语音识别系统具有更高的鲁棒性，并且对保留数据集的泛化效果更好，而单一来源训练的模型则不然。这些工作通过结合尽可能多的现有高质量语音识别数据集来实现这一点。然而，目前仅有少量此类数据可供使用。SpeechStew（Chan等人，2021）混合了7个现有数据集，总共有5140小时的监督数据。尽管不可忽视，但与前面提到的使用了100万小时未标记语音数据的工作相比，这仍然是微不足道的。

鉴于现有高质量监督数据集的限制大小，最近的努力已经为语音识别创建了更大的数据集。通过放宽对黄金标准的人工验证转录的要求，Chen等人（2021）和Galvez等人（2021）利用了复杂的自动化流水线，将弱监督语音识别扩展到了10,000和30,000小时的更嘈杂的训练数据。在质量和数量之间的权衡通常是正确的选择。尽管到目前为止对于语音识别而言尚未有足够的研究，但最近在计算机视觉领域的工作表明，从ImageNet（Russakovsky等人，2015）等黄金标准众包数据集转向更大但弱监督的数据集显着提高了模型的鲁棒性和泛化性。

然而，这些新数据集仅比现有高质量数据集的总和大几倍，并且仍然远远小于以前的无监督工作。在这项工作中，我们将弱监督语音识别扩展了一个数量级，达到了68万小时的标记音频数据。我们称之为Whisper。我们展示了以这种规模训练的模型在现有数据集上的零样本迁移效果良好，无需任何数据集特定的微调即可实现高质量的结果。

除了规模之外，我们的工作还专注于将弱监督预训练的范围从仅英语语音识别扩展到多语言和多任务。在这680,000小时的音频中，117,000小时涵盖了其他96种语言。该数据集还包括125,000小时的X→en翻译数据。我们发现对于足够大的模型，联合多语言和多任务训练没有缺点，甚至还有好处。

我们的工作表明，到目前为止对于语音识别来说，简单扩展弱监督预训练的价值尚未得到充分重视。我们在没有自我监督或自我训练技术的情况下实现了这些结果，这些技术是最近大规模语音识别工作的主要内容。为了为进一步研究鲁棒语音识别提供基础，我们在以下网址发布了推理代码和模型：https://github.com/openai/whisper。

## 2 Approach

### 2.1. 数据处理
跟随近期利用互联网规模的文本来训练机器学习系统的趋势，我们采取了一种极简主义的数据预处理方法。与很多语音识别工作相比，我们训练Whisper模型来预测原始文本的转录，而无需进行任何重要的标准化，依靠序列到序列模型的表现力来学习如何映射话语和其转录形式之间的关系。这简化了语音识别流程，因为它消除了需要单独进行逆文本规范化步骤以生成自然的转录的需求。

我们从互联网上配对了音频和转录来构建数据集。这导致了一个非常多样化的数据集，涵盖了来自许多不同环境、录制设置、说话者和语言的广泛音频分布。尽管音频质量的多样性有助于训练模型具有鲁棒性，但转录质量的多样性并不同样有益。初步检查显示原始数据集中存在大量低质量的转录。为了解决这个问题，我们开发了几种自动过滤方法来提高转录的质量。

互联网上的许多转录实际上并不是人工生成的，而是现有ASR系统的输出。最近的研究表明，训练混合人类和机器生成数据集的数据可以显著降低翻译系统的性能（Ghorbani等人，2021）。为了避免学习“转录语”，我们开发了许多启发式方法来检测和删除训练数据集中的机器生成的转录。许多现有ASR系统仅输出书面语言的有限子集，从而消除或标准化掉从仅从音频信号中难以预测的方面，例如复杂的标点符号（感叹号、逗号和问号）、格式化的空格（如段落）或样式方面，例如大写。全大写或全小写的转录极有可能不是由人类生成的。虽然许多ASR系统包括某种程度的逆文本规范化，但它通常是简单或基于规则的，仍然可以从其他未处理的方面进行检测，例如永远不包含逗号。

我们还使用了音频语言检测器，它是通过在VoxLingua107（Valk＆Alumäe，2021）上对原型数据集的原型版本进行微调而创建的，以确保所说的语言与转录的语言相匹配。如果两者不匹配，我们就不将（音频，转录）对作为语音识别训练示例包含在数据集中。如果转录语言是英语，则我们会将这些对作为X→en语音翻译训练示例添加到数据集中。我们使用模糊去重转录文本以减少重复和自动生成内容在训练数据集中的数量。

我们将音频文件分成30秒的片段，并与该时间段内出现的转录的子集配对。我们对所有音频进行训练，包括没有语音的片段（尽管以子采样概率），并将这些片段用作语音活动检测的训练数据。

在对初始模型进行训练后，我们进行了额外的过滤步骤，汇总了有关其在训练数据源上的错误率的信息，并对这些数据源进行了手动检查，按高错误率和数据源大小的组合进行排序，以有效地识别和删除低质量的数据源。这次检查显示，有大量部分转录或对齐/错位不良的转录，以及仍然存在的低质量机器生成字幕，这些启发式字幕过滤方法未能检测到。

为避免污染，我们在训练数据集和我们认为有更高重叠风险的评估数据集之间进行了转录级的去重，即TED-LIUM 3（Hernandez等人，2018）。


### 2.2. 模型

由于我们工作的重点是研究大规模监督预训练对语音识别的能力，我们使用现成的架构，以避免将我们的发现与模型改进混淆。我们选择了编码器-解码器Transformer（Vaswani等人，2017），因为这种架构已经被充分验证可以可靠地扩展。所有音频都被重新采样为16,000 Hz，然后在25毫秒的窗口上以10毫秒的步长计算出80通道的对数幅度梅尔频谱图表示。对于特征归一化，我们将输入全局缩放到-1到1之间，并在预训练数据集上具有近似零均值。编码器通过由两个卷积层组成的小的干扰进行输入表示的处理，卷积层的滤波宽度为3，使用GELU激活函数（Hendrycks＆Gimpel，2016），其中第二个卷积层的步长为二。然后在干扰的输出上添加了正弦位置嵌入，然后应用编码器Transformer块。transformer使用预激活残差块（Child等人，2019），并对编码器输出应用最终层归一化。解码器使用学习的位置嵌入和绑定的输入-输出标记表示（Press＆Wolf，2017）。编码器和解码器具有相同的宽度和Transformer块数。图1总结了模型架构。


<a>![](/img/whisper/1.png)</a>
**图1. 我们方法的概述。一个序列到序列的Transformer模型被训练用于许多不同的语音处理任务，包括多语言语音识别、语音翻译、口语识别和语音活动检测。所有这些任务都以一个要由解码器预测的token序列的形式进行联合表示，从而允许一个单一的模型取代传统语音处理流程中的许多不同阶段。多任务训练格式使用一组特殊的token，这些token用作任务说明符或分类目标，如第2.3节进一步解释的那样。**

我们对英语模型使用了与GPT-2相同的字节级BPE文本标记器（Sennrich等人，2015; Radford等人，2019），并针对多语言模型重新训练了词汇表（但保持相同大小），以避免在其他语言上出现过度碎片化，因为GPT-2 BPE词汇表仅适用于英语。

 
### 2.3. 多任务格式

虽然预测给定音频片段中所说的单词是完整语音识别问题的核心部分，并且在研究中得到了广泛研究，但这并不是唯一的部分。一个完整的语音识别系统可能涉及许多额外的组件，如语音活动检测、说话者分割和逆文本归一化。这些组件通常单独处理，导致围绕核心语音识别模型的系统相对复杂。为了减少这种复杂性，我们希望有一个单一的模型执行整个语音处理流水线，而不仅仅是核心识别部分。这里的一个重要考虑因素是模型的接口。可以对相同的输入音频信号执行许多不同的任务：转录、翻译、语音活动检测、对齐和语言识别是一些例子。

为了使这种一对多映射与单一模型配合工作，需要某种形式的任务规范。我们使用一种简单的格式将所有任务和条件信息指定为输入token序列给解码器。由于我们的解码器是一个音频条件语言模型，我们还训练它在希望它能学会使用更长范围的文本上下文来解决模糊音频的情况下对转录文本的历史进行条件化。具体而言，我们以一定的概率将当前音频段之前的转录文本添加到解码器的上下文中。我们用一个 \<\|prediction\|> token指示预测的开始。

首先，我们预测正在说的语言，这由我们训练集中每种语言的唯一token表示（共99种）。这些语言目标来自前述的 VoxLingua107 模型。在音频段中没有语音的情况下，模型被训练预测一个 \<\|silence\|> token来指示这一点。接下来的token指定了任务（转录或翻译）以及有无 \<\|no-timestamps\|> token。在这一点上，任务和所需格式已完全指定，输出开始。对于时间戳的预测，我们预测相对于当前音频段的时间，将所有时间量化到最近的20毫秒，这与 Whisper 模型的本地时间分辨率匹配，并为每个时间添加额外的token到我们的词汇表中。我们将它们的预测与字幕token交错：开始时间token在每个字幕文本之前预测，结束时间token在之后预测。当最终的转录片段仅部分包含在当前的30秒音频块中时，我们仅为时间戳模式下的片段预测其开始时间token，以指示随后的解码应在与该时间对齐的音频窗口上执行，否则(\<\|no-timestamps\|>)我们将音频截断以不包括该段。最后，我们添加一个 \<\|end-of-output\|> token。我们只遮蔽之前上下文文本的训练损失，并训练模型预测所有其他token。请参见图1，了解我们格式和训练设置的概述。


### 2.4. 训练细节

我们训练了一系列不同大小的模型，以研究 Whisper 的扩展特性。请参见表1以获取概述。

<a>![](/img/whisper/2.png)</a>
*表1. Whisper模型系列的架构细节。*

我们使用FP16进行数据并行训练，使用动态损失缩放和激活检查点（Griewank & Walther, 2000; Chen等，2016）。模型使用AdamW（Loshchilov & Hutter, 2017）和梯度范数裁剪（Pascanu等，2013）进行训练，并且使用线性学习率衰减，在前2048次更新后衰减到零。使用批量大小为256个段，模型进行220次更新训练，这相当于对数据集进行两到三次遍历。由于只进行了少数几轮训练，过拟合并不是一个很大的问题，我们没有使用任何数据增强或正则化，而是依靠如此庞大数据集中包含的多样性来鼓励泛化和鲁棒性。请参见附录F以获取完整的训练超参数。$^3$

在早期开发和评估中，我们观察到 Whisper 模型有一种倾向，即对说话者的名称进行可信但几乎总是不正确的猜测。这是因为预训练数据集中的许多转录包含正在讲话的人的姓名，鼓励模型尝试预测它们，但这种信息只有在最近30秒的音频上下文中很少推断出来。为了避免这种情况，我们对不包括说话者标注的转录子集对 Whisper 模型进行简要微调，以消除这种行为。

**注$^3$**：在Whisper最初发布之后，我们额外训练了一个更大的模型（标记为V2），训练时长增加了2.5倍，同时添加了SpecAugment (Park et al., 2019), Stochastic Depth (Huang et al., 2016), and BPE Dropout (Provilkov et al., 2019) 进行正则化。除非另有说明，否则报告的结果已更新为这个改进的模型。


## 3 实验

### 3.1. 零样本评估

Whisper的目标是开发一个单一的稳健语音处理系统，它能够可靠地工作，无需对特定数据集进行微调即可在特定分布上实现高质量的结果。为了研究这种能力，我们重新使用了一系列现有的语音处理数据集，以检查Whisper是否能够很好地跨领域、跨任务和跨语言进行泛化。我们不使用这些数据集的标准评估协议，其中包括训练集和测试集分割，而是在零样本设置中评估Whisper，不使用任何这些数据集的训练数据，以便我们测量广泛的泛化能力。

### 3.2. 评估指标

语音识别研究通常根据单词错误率（WER）指标来评估和比较系统。然而，WER基于字符串编辑距离，对模型输出与参考转录之间的所有差异进行惩罚，包括转录样式上的无害差异。因此，即使系统输出的转录在人类看来是正确的，WER仍然可能很大，因为存在微小的格式差异。虽然这对所有的转录者都是一个问题，但对于像Whisper这样的零样本模型，尤其严重，因为它们没有观察到任何特定数据集转录格式的例子。

这并不是一个新的观察结果；开发与人类判断更相关的评估指标是一个活跃的研究领域，尽管有一些有希望的方法，但在语音识别领域还没有看到广泛的应用。我们选择通过在计算WER之前对文本进行广泛标准化来解决这个问题，以最小化对非语义差异的惩罚。我们的文本规范化器是通过迭代手动检查开发的，以识别出Naive WER对于无害差异的Whisper模型进行惩罚的常见模式。附录C包含完整的细节。对于几个数据集，我们观察到WER的降低多达50%，通常是由于数据集的参考转录将缩写词与单词用空格分隔这样的特殊情况。我们警告说，这种开发过程存在过拟合到Whisper模型的转录风格的风险，我们在第4.4节中对此进行了调查。我们正在发布我们的文本规范化器的代码，以便进行轻松比较，并帮助其他人研究在分布外设置中语音识别系统的性能。

### 3.3. 英语语音识别

2015年，Deep Speech 2（Amodei等，2015）报告了一个语音识别系统，在转录LibriSpeech test-clean分割时与人类水平的性能相匹配。作为分析的一部分，他们得出结论：“考虑到这一结果，我们怀疑在没有进一步的域适应的情况下，通用语音系统很难进一步改进清晰的朗读语音。”然而，七年后，LibriSpeech test-clean上的SOTA WER又从他们的5.3%降至1.4%（Zhang等，2021），远低于他们报告的5.8%的人类级错误率。尽管在保留但分布内的数据上性能进一步提高的这种巨大且意想不到的进步，但在其他设置中使用LibriSpeech训练的语音识别模型的性能仍然远高于人类错误率。是什么解释了这种分布内报告的超人表现与分布外的亚人表现之间的差距？

我们怀疑，人类和机器行为之间的这种差距的很大一部分是由于在测试集上测量人类和机器性能时混淆了不同的能力。这种说法乍一听可能令人困惑；如果人类和机器都在同一测试中，那么如何可能测试到不同的技能呢？区别不在于测试，而在于它们是如何进行训练的。人们经常被要求在几乎没有或没有特定数据分布监督的情况下执行任务。因此，人类表现是对分布外泛化的一种度量。但是，机器学习模型通常在接受来自评估分布的大量监督后进行评估，这意味着机器性能实际上是对分布内泛化的一种度量。虽然人类和机器都在同一测试数据上进行评估，但由于训练数据的不同，两者之间测量到了两种完全不同的能力。

Whisper模型是在广泛而多样的音频分布上训练的，并且在零样本设置中进行评估，可能比现有系统更好地匹配人类行为。为了研究这是否属实（或者机器和人类性能之间的差异是否由尚未理解的因素造成），我们可以将Whisper模型与人类性能和标准微调的机器学习模型进行比较，并检查它们更接近哪一个。

为了量化这种差异，我们同时考虑了整体的鲁棒性（即在许多分布/数据集上的平均性能）和有效的鲁棒性——由Taori等人（2020年）提出，它衡量了参考数据集（通常是分布内的）与一个或多个分布外数据集之间的预期性能差异。具有高有效鲁棒性的模型在分布外数据集上的表现优于预期，其性能与所有数据集上的平均性能相等。对于我们的分析，我们使用LibriSpeech作为参考数据集，因为它在现代语音识别研究中起着核心作用，并且有许多在其上训练的发布模型，这使得能够表征鲁棒性行为。我们使用12个其他学术语音识别数据集的套件来研究分布外行为。关于这些数据集的详细信息可以在附录A中找到。

<a>![](/img/whisper/3.png)</a>
**图2. 零样本的Whisper模型缩小了与人类鲁棒性之间的差距。尽管在LibriSpeech dev-clean上与人类匹配或表现优越，但监督的LibriSpeech模型在其他数据集上的错误数量大约是人类的两倍，显示了它们的脆弱性和缺乏鲁棒性。然而，零样本的Whisper模型的估计鲁棒性前沿包括了这个特定人类的95%置信区间。**

我们的主要发现总结在图2和表2中。尽管最佳的零样本Whisper模型在LibriSpeech清晰测试集的WER相对不起眼，为2.5，大致与现代监督基线或2019年中期的最先进水平相当，零样本Whisper模型在鲁棒性特性上与监督的LibriSpeech模型有很大不同，并且在其他数据集上超过了所有基准的LibriSpeech模型。即使是最小的零样本Whisper模型，只有3900万参数，LibriSpeech测试清洁上的WER为6.7，当在其他数据集上评估时，它也大致与最佳的监督LibriSpeech模型竞争。与图2中的人类进行比较时，最佳的零样本Whisper模型大致匹配其准确性和鲁棒性。有关鲁棒性显着提高的详细分析，表2将最佳的零样本Whisper模型的性能与在LibriSpeech测试清洁上最接近它的监督LibriSpeech模型进行了比较。尽管它们在参考分布上的性能非常接近，但零样本Whisper模型在其他语音识别数据集上的平均相对错误减少了55.2%。

<a>![](/img/whisper/4.png)</a>
**表2. 在不同数据集上有效鲁棒性的详细比较。尽管两个模型在LibriSpeech上的性能相差不到0.1%，但零样本的Whisper模型在其他数据集上的表现比其在LibriSpeech上的表现更好，平均减少了55.2%的错误。在应用我们的文本规范化器后，两个模型的结果以单词错误率（WER）报告。**

这一发现表明，强调模型的零样本和分布外评估，特别是在试图与人类性能进行比较时，可以避免通过误导性比较夸大机器学习系统的能力。


### 3.4. 多语言语音识别

为了与以往的多语言语音识别工作进行比较，我们在两个低数据基准上报告结果：Multilingual LibriSpeech (MLS)（Pratap等，2020b）和VoxPopuli（Wang等，2021），见表3。


<a>![](/img/whisper/6.png)</a>
**表3. 多语言语音识别性能。零样本的Whisper在多语言LibriSpeech（MLS）上提升了性能，但在VoxPopuli上仍然显著落后于Maestro、XLS-R和mSLAM。**

Whisper在Multilingual LibriSpeech上表现良好，在零样本设置下优于XLS-R（Babu等，2021）、mSLAM（Bapna等，2022）和Maestro（Chen等，2022b）。我们需要注意的是，我们在此结果中使用了一个简单的文本标准化器，这防止了直接比较或宣称SOTA性能。然而，在VoxPopuli上，Whisper明显表现不佳，仅超过了原始论文中的VP-10K+FT基线。我们怀疑Whisper模型在VoxPopuli上的表现不佳可能是由于其他模型将此分布作为其无监督预训练数据的主要来源，并且该数据集具有大量的受监督数据，这有利于微调。虽然MLS每种语言有10小时的训练数据，但VoxPopuli每种语言的平均训练数据量大约是MLS的10倍。

这两个基准有些狭窄，因为它们仅包括15种独特的语言，几乎所有这些语言都属于印欧语系，其中许多是高资源语言。这些基准仅提供有限的覆盖范围和研究Whisper模型多语言能力的空间，其中包括75种语言的语音识别训练数据。为了更广泛地研究Whisper的性能，我们还报告了在Fleurs数据集上的性能（Conneau等，2022）。特别地，我们对于研究我们对于给定语言的训练数据量和该语言的下游零样本性能之间的关系感兴趣。

我们在图3中可视化了这种关系。我们发现，单词错误率的对数与每种语言的训练数据量的对数之间有很强的平方相关系数，为0.83。检查对这些对数-对数值进行线性拟合的回归系数，得出的估计是，每增加16倍的训练数据，WER就减半。我们还观察到，根据这一趋势，许多最大的异常值中，表现不佳的语言是具有独特文字并且与占训练数据集大多数的印欧语言更为疏远的语言，例如希伯来语（HE）、泰卢固语（TE）、中文（ZH）和韩文（KO）。这些差异可能是由于语言间的迁移不足、我们的字节级BPE分词器与这些语言不匹配，或者数据质量的变化所致。

<a>![](/img/whisper/5.png)</a>
**图3  预训练监督量与下游语音识别性能的相关性。对于给定语言的预训练语音识别数据量与该语言在Fleurs上的零样本性能之间存在很强的相关性。**


### 3.5. 翻译

我们通过衡量Whisper模型在CoVoST2的X→en子集上的性能来研究其翻译能力（Wang等，2020b）。我们与Maestro、mSLAM和XLS-R进行比较，它们是以前表现最佳的工作。我们在没有使用任何CoVoST2训练数据的情况下，实现了29.1 BLEU的新的零样本最先进。我们将这归因于我们预训练数据集中这些语言的X→en翻译数据的68,000小时，尽管存在噪音，但远远大于CoVoST2中X→en翻译的861小时的训练数据。由于Whisper评估是零样本的，因此它在CoVoST2的资源最低的分组上表现特别好，比mSLAM提高了6.7 BLEU。相反，最佳的Whisper模型在最高资源语言的平均性能上实际上并未超过Maestro和mSLAM。


<a>![](/img/whisper/7.png)</a>
**表4. X→en语音翻译性能。零样本的Whisper在CoVoST2的整体、中等和低资源设置上优于现有模型，但与之前的直接监督工作相比，在高资源语言上仍然表现中等。**

为了对更广泛的语言集进行额外分析，我们还重新利用了Fleurs，这是一个语音识别数据集，作为一个翻译数据集。由于每种语言都为相同的句子进行转录，我们使用英文转录作为参考翻译。在图4中，我们可视化了每种语言的翻译训练数据量与Fleurs上的零样本BLEU分数之间的相关性。虽然随着训练数据的增加而改善的趋势很明显，但平方相关系数远远低于语音识别的0.83，仅为0.24。我们怀疑这在一定程度上是由于训练数据的噪音引起的，这是由于音频语言识别中的错误。例如，威尔士语（CY）是一个异常值，在仅有13 BLEU的情况下远低于预期的性能，尽管据称拥有9,000小时的翻译数据。这么大量的威尔士语翻译数据令人惊讶，在翻译数据中排名第4，并且领先于世界上一些最常用的语言，如法语、西班牙语和俄语。检查结果显示，据称是威尔士语的大部分翻译数据实际上是英文音频与英文字幕，其中英文音频被语言识别系统错误地识别为威尔士语，导致其被包括为翻译训练数据而不是转录数据，这符合我们的数据集创建规则。


### 3.6. 语言识别

为了评估语言识别，我们使用了Fleurs数据集（Conneau等，2022）。在这里，Whisper的零样本性能与以前的监督工作相比并不具竞争力，并且比监督SOTA表现低了13.6%。然而，在Fleurs上，Whisper的语言识别处于严重劣势，因为Whisper数据集中没有Fleurs中102种语言的20种的训练数据，这将准确率上限设定为80.4%。在82种重叠语言中，最佳的Whisper模型实现了80.3%的准确率。

<a>![](/img/whisper/8.png)</a>
**表5. 语言识别性能。零样本的Whisper在语言识别方面的准确率与Fleurs数据集中以前的监督结果不具竞争力。这在一定程度上是由于Whisper在20种Fleurs语言中没有训练数据，因此受到严重惩罚的原因。**

### 3.7. 对加性噪声的鲁棒性

我们通过测量将白噪声或来自音频降级工具包（Mauch & Ewert, 2013）的酒吧噪声添加到音频时的词错误率（WER），来测试Whisper模型和14个LibriSpeech训练模型的噪声鲁棒性。酒吧噪声代表了更自然的嘈杂环境，其中包括典型拥挤餐馆或酒吧中的环境噪音和不清晰的交谈声。在这14个模型中，有12个是在LibriSpeech上进行了预训练和/或微调的，另外两个是NVIDIA STT模型，是在类似于SpeechStew的先前工作中包含LibriSpeech的混合数据集上训练的。与给定信噪比（SNR）相对应的加性噪声水平是根据各个示例的信号功率计算的。图5显示了随着加性噪声的强度增加，语音识别性能如何下降。在低噪声（40 dB SNR）下，有许多模型表现优于我们的零样本性能，这并不奇怪，因为这些模型主要是在LibriSpeech上训练的，但是随着噪声变得更加强烈，所有模型都迅速下降，表现比Whisper模型在SNR低于10 dB的酒吧噪声下更差。这展示了Whisper对噪声的鲁棒性，特别是在更自然的分布转移，如酒吧噪声下。

<a>![](/img/whisper/9.png)</a>
**图5. 在加性白噪声（左侧）和酒吧噪声（右侧）下，LibriSpeech测试集-干净的词错误率（WER）随信噪比（SNR）变化的情况。LibriSpeech训练模型的准确率下降速度比最佳的Whisper模型（F）更快。NVIDIA STT模型（•）在低噪声下表现最佳，但在高噪声下被Whisper超越（SNR < 10 dB）。在低噪声下第二好的模型（H）仅在LibriSpeech上进行微调，且下降速度更快。**


### 3.8. 长篇转录

Whisper模型是在30秒音频块上进行训练的，无法一次处理更长的音频输入。这在由短语言组成的大多数学术数据集中不是问题，但在现实世界的应用中会带来挑战，因为这些应用通常需要转录几分钟或几小时长的音频。我们开发了一种策略，通过连续转录音频的30秒片段并根据模型预测的时间戳移动窗口来执行长音频的缓冲转录。我们观察到，根据模型预测的重复性和对数概率来进行束搜索和温度调度对于可靠地转录长音频至关重要。完整的过程在第4.5节中描述。

我们评估了七个数据集上的长篇转录性能，这些数据集包括各种长度和录制条件的语音记录，以尽可能覆盖多样化的数据分布。其中包括TED-LIUM3的长篇调整版（Hernandez等，2018），其中每个示例都是一个完整的TED演讲，从The Late Show with Stephen Colbert提取的充满行话的片段（Meanwhile），在在线博客中用作ASR基准的视频/播客集合（Rev16和Kincaid46），收益电话的录音（Del Rio等，2021），以及来自区域非裔美国人语言语料库（CORAAL）的完整采访（Gunter等，2021）。有关长篇数据集的完整详情，请参阅附录A。


<a>![](/img/whisper/10.png)</a>
**图6. 在长篇转录中，Whisper与最先进的商业和开源ASR系统具有竞争力。比较了七个长篇数据集上六个ASR系统的词错误率分布，输入长度范围从几分钟到几小时不等。箱形图显示了每个示例的WER四分位数，并在每个箱子上标注了每个数据集的汇总WER。我们的模型在所有数据集上都优于最佳的开源模型（NVIDIA STT），在大多数情况下，也优于商业ASR系统。**

我们将性能与开源模型以及4个商业ASR服务进行了比较。结果总结如图6所示，显示了Whisper和4个商业ASR服务的词错误率分布，以及NeMo工具包（Kuchaiev等，2019）中性能最佳的NVIDIA STT Conformer-CTC Large模型的结果。所有商业ASR服务均使用截至2022年9月1日的默认英语转录设置进行查询，对于NVIDIA STT模型，我们使用了其FrameBatchASR类中的缓冲推理实现以实现长篇转录。结果显示，Whisper在大多数数据集上的表现优于所比较的模型，特别是在Meanwhile数据集上，该数据集中包含大量不常见的词汇。此外，我们注意到一些商业ASR系统可能已经在一些这些公开可用的数据集上进行了训练，因此这些结果可能无法准确反映系统的相对鲁棒性。

### 3.9 和人类对比

由于模糊或不清晰的语音以及标注错误，每个数据集中都存在不同水平的不可减少的错误，仅通过ASR系统的WER指标很难理解每个数据集存在多少改进空间。为了量化Whisper的性能与人类性能的接近程度，我们从Kincaid46数据集中选取了25个录音，并使用5个服务获取由专业转录员生成的转录文本，其中一个提供计算机辅助转录，另外四个完全由人工转录。音频选择涵盖了各种录音条件，如脚本和非脚本广播、电话和VoIP通话以及会议。图7显示了每个示例的WER分布和25个录音的汇总WER，其中计算机辅助服务的汇总WER最低，比Whisper的低1.15个百分点，而纯人工性能仅比Whisper的好几个百分点。这些结果表明，Whisper的英文ASR性能虽然不是完美的，但非常接近人类水平的准确性。

<a>![](/img/whisper/11.png)</a>
**图7。Whisper的性能接近专业人工转录员的水平。该图显示了Kincaid46数据集中的25个录音的WER分布，由Whisper、图6中相同的4个商业ASR系统（A-D）、一个计算机辅助人工转录服务（E）和4个人工转录服务（F-I）转录。箱线图上叠加了表示各个录音WER的点，每个箱子上标注了25个录音的汇总WER。**



### 4 结果消融分析

### 4.1. 模型规模

弱监督训练方法的许多优势在于它们有望使用比传统监督学习中更大得多的数据集。然而，这也带来了一个成本，即可能使用的数据比黄金标准监督的数据要嘈杂得多，质量也较低。这种方法的一个担忧是，虽然开始时看起来很有希望，但是在这种数据上训练的模型的性能可能会饱和到数据集的固有质量水平，这可能远远低于人类水平。一个相关的担忧是，随着用于训练数据集的容量和计算量的增加，模型可能会学会利用数据集的特异性，并且它们对于对分布数据的鲁棒泛化能力甚至可能会下降。

为了检查是否存在这种情况，我们研究了Whisper模型的zero-shot泛化性能与模型规模的关系。我们的分析总结如图8所示。除了英语语音识别之外，在多语种语音识别、语音翻译和语言识别领域，性能随着模型规模的增加而持续提高。对于英语语音识别的收益递减可能是由于接近人类水平性能导致的饱和效应，正如第3.9节的分析所示。

<a>![](/img/whisper/12.png)</a>
**图8. 随着模型规模的增加，zero-shot Whisper性能在任务和语言上可靠地扩展。浅色阴影线代表个别数据集或语言，显示性能比总体性能的平滑趋势更为多样化。大型V2用虚线橙色线条标识，因为它包含了在此分析中较小模型中不存在的几个变化。**


### 4.2. 数据集扩展

拥有 68 万小时的标记音频，Whisper 数据集是有史以来在监督式语音识别中创建的最大数据集之一。原始数据集大小对 Whisper 的性能究竟有多重要？ 为了研究这一点，我们训练了一系列中等大小的模型，使用原数据集大小的 0.5%、1%、2%、4% 和 8% 的子采样版本，并将它们的性能与在完整数据集上训练的相同中等大小模型进行比较。基于验证损失的早停法被用于选择每个数据集大小的模型检查点。评估是在参数的指数移动平均估计上进行的（Polyak & Juditsky, 1992），使用平滑率为 0.9999，以帮助减少由于早停法导致子采样数据集上训练的模型的学习率未完全衰减到零而产生的影响。英语、多语言语音识别和 X→en 翻译的性能报告在表 6 中。

<a>![](/img/whisper/13.png)</a>
**表 6. 随着数据集大小的增加，性能提升。英语语音识别性能是对 12 个数据集的平均值，而多语言语音识别报告了 Fleurs 中重叠语言子集的性能，X→en 翻译报告了 CoVoST2 上的平均 BLEU。数据集大小以小时为单位报告。**

所有数据集大小的增加都导致所有任务的性能改善，尽管我们在任务和大小之间看到了显著的改善速率的变化。在英语语音识别方面，性能从 3,000 小时提高到 13,000 小时，然后在 13,000 小时至 54,000 小时之间明显放缓。使用完整数据集，对应着大小增加了 12.5 倍，结果只有进一步 1 点的 WER 下降。这与英语语音识别模型大小扩展的收益递减相呼应，并且类似地可能可以通过接近人类水平的性能来解释。

多语言语音识别的 WER 改善遵循幂律趋势，直到 54,000 小时，然后偏离这个趋势，在增加到完整数据集大小时只进一步提高了 7 点。对于 X→en 翻译，当训练数据少于 7,000 小时时，性能几乎为零，然后在 54,000 小时之间遵循大致对数线性的改善趋势，之后在进一步扩展到完整数据集大小时也显示出收益递减的趋势。

在从 54,000 小时移动到我们的完整数据集大小 680,000 小时时，跨任务的收益递减的一般趋势可能表明，目前最好的 Whisper 模型相对于数据集大小来说还未充分训练，性能可以通过更长的训练和更大的模型组合进一步提高。这也可能表明，我们正接近语音识别数据集大小扩展的性能改善的尽头。需要进一步分析以表征语音识别的“扩展定律”，以便在这些解释之间做出选择。

### 4.3. 多任务和多语言迁移

在训练单个模型同时处理多个任务和语言时，一个潜在的担忧是可能会出现负迁移，即多个任务之间的学习干扰导致性能不如只针对单一任务或语言进行训练时所达到的性能。为了调查是否出现了这种情况，我们比较了仅在英语语音识别上训练的模型与我们标准的多任务和多语言训练设置下的性能，并测量它们在我们一系列零迁移英语语音识别基准测试中的平均性能。我们根据用于英语语音识别任务的 FLOPs 量进行了调整，因为在联合训练设置中，只有 65% 的计算用于这个任务；否则，分析结果可能会受到任务的欠训练的干扰，与同等大小的仅英语模型相比。

<a>![](/img/whisper/14.png)</a>
**图 9. 随着规模的增大，多任务和多语言迁移效果提高。对于小型模型，在多任务和多语言设置下联合训练的英语语音识别性能会下降。然而，多语言和多任务模型更多地受益于规模，并最终优于仅在英语数据上训练的模型。显示了 95% 自举估计置信区间。**

我们的结果在图 9 中可视化显示，对于用中等计算量训练的小型模型，确实存在任务和语言之间的负迁移：联合模型的性能低于同等计算量训练的仅英语模型。然而，多任务和多语言模型的规模化效果更好，在我们最大的实验中，它们的性能优于它们的仅英语对应模型，表现出了来自其他任务的正迁移。对于我们最大的实验，联合模型即使在不调整每个任务所花计算量的情况下，也略优于仅英语模型。

### 4.4. 文本标准化

由于我们与 Whisper 一起开发了我们的文本标准化方法来排除无害的单词错误，存在一个风险，即我们的标准化器过度拟合于修复 Whisper 的特殊性，而不是解决转录中的一般变化。为了检查这一点，我们比较了使用我们的标准化器与 FairSpeech 项目（Koenecke 等人，2020）独立开发的标准化器的 Whisper 的性能。在图 10 中，我们可视化了差异。在大多数数据集上，这两种标准化器的性能相似，与 Whisper 和其他开源模型之间的 WER 减少没有显著差异，然而在某些数据集上，特别是 WSJ、CallHome 和 Switchboard 上，我们的标准化器显著降低了 Whisper 模型的 WER。减少的差异可以追溯到ground-truth使用的不同格式以及两个标准化器是如何惩罚它们的。例如，在 CallHome 和 Switchboard 上，我们的标准化器不惩罚常见的英文缩写的差异，如“you're”和“you are”，在 WSJ 中，我们的标准化器标准化了数字和货币表达式的书面和口头形式，如“sixty-eight million dollars”与“$68 million”。

<a>![](/img/whisper/15.png)</a>
**图 10. 在大多数数据集上，与 FairSpeech 的标准化器相比，我们的文本标准化器对减少 Whisper 模型和其他开源模型之间的 WER 有类似的效果。对于每个数据集，箱形图显示了我们评估套件中不同模型的相对 WER 减少的分布，表明使用我们的文本标准化器通常会导致比 FairSpeech 更低的 WER。在一些数据集上，我们的标准化器显著降低了 WER，而且对 Whisper 模型的影响更大，例如 CallHome 和 Switchboard，这些数据集在ground-truth中包含许多缩写，以及 WSJ，其中包含许多数字表达式。**

### 4.5. 可靠的长篇转录策略

使用 Whisper 进行长篇音频转录依赖于准确预测时间戳token，以确定将模型的 30 秒音频上下文窗口移动的量，而一个窗口中的不准确转录可能会对后续窗口的转录产生负面影响。我们已经开发了一组启发式方法，可以帮助避免长篇转录的失败情况，这些方法在第 3.8 和 3.9 节中报告的结果中得到了应用。首先，我们使用 5 条束搜索，将对数概率作为评分函数，以减少在贪婪解码中更频繁出现的重复循环。我们从温度为 0 开始，即始终选择概率最高的token，并在平均对数概率低于 -1 或生成的文本的 gzip 压缩率高于 2.4 时将温度增加 0.2，直到达到 1.0。当应用温度低于 0.5 时，将前一个窗口的转录文本作为前文条件进一步改善了性能。我们发现，仅使用 \<\|nospeech\|>token的概率无法区分没有语音的片段，但将没有语音的概率阈值设置为 0.6 并将平均对数概率阈值设置为 -1 使 Whisper 的语音活动检测更可靠。最后，为了避免模型忽略输入中的前几个单词的失败模式，我们限制了初始时间戳token在 0.0 到 1.0 秒之间。表 7 显示，逐步添加上述每个干预措施总体上降低了整体 WER，但在整个数据集中效果并不均匀。这些启发式方法作为模型嘈杂预测的解决方法，需要进一步研究来进一步提高长篇转录的可靠性。


<a>![](/img/whisper/16.png)</a>
**表7. 当额外的解码启发式方法被采用时，长篇转录性能逐步提升。关于每个干预措施的详细信息请参见第4.5节。**


## 5 Related Work


**扩展语音识别**

语音识别研究的一个持续主题是记录扩展计算、模型和数据集的好处。早期将深度学习应用于语音识别的工作发现，通过增加模型的深度和大小，并利用GPU加速来使训练这些较大模型变得可行，可以改善性能。进一步的研究表明，深度学习方法对语音识别的益处随着数据集大小的增加而增加，当仅使用3小时的TIMIT训练数据进行电话识别时，性能仅与之前的GMM-HMM系统竞争，而在训练了2000小时的Switchboard数据集后，其错误率降低了30%。Liao等人(2013)是一个早期的例子，他们利用弱监督学习来增加深度学习语音识别数据集的规模，使其超过1000小时。这些趋势延续到了Deep Speech 2(Amodei等人，2015)，它是一个显着的系统，通过在16个GPU上进行高吞吐量的分布式训练，并扩展到12000小时的训练数据，同时在这一规模上持续改进。通过利用半监督的预训练，Narayanan等人(2018)能够进一步扩大数据集的规模，并研究在162,000小时标记音频上的训练。更近期的工作探讨了百亿参数模型(Zhang等人，2020)和使用高达100万小时的训练数据(Zhang等人，2021)。

**多任务学习**

多任务学习(Caruana，1997)已经研究了很长时间。在语音识别中，多语言模型的研究已经有十多年的历史(Schultz＆Kirchhoff，2006)。探索使用单个模型进行多任务学习的NLP方面的启发性和基础性工作是Collobert等人(2011)。Luong等人(2015)研究了序列到序列框架中的多任务学习，使用多个编码器和解码器。Johnson等人(2017)首次在机器翻译中展示了具有共享编码器/解码器架构的语言代码，消除了分开编码器和解码器的需要。这种方法进一步简化为McCann等人(2018)的“文本到文本”框架，并在Radford等人(2019)和Raffel等人(2020)的大型变压器语言模型的成功工作中得到推广。Toshniwal等人(2018)证明了使用单个模型同时训练现代深度学习语音识别系统的多种语言，并且Pratap等人(2020a)显著地将这一工作线扩展到50种语言，并使用了一个拥有十亿参数的模型。MUTE(Wang等人，2020c)和mSLAM(Bapna等人，2022)研究了文本和语音语言任务的联合训练，展示了它们之间的转移效果。

**鲁棒性**

关于模型如何有效地转移以及它们对分布转移和其他类型的扰动的鲁棒性的问题，长期以来一直受到研究，并且正在许多机器学习领域积极进行研究。Torralba＆Efros(2011)在十多年前强调了机器学习模型在数据集之间的泛化不足。许多其他作品已经表明，并不断强调，尽管在IID测试集上性能很高，但当在稍微不同的设置中评估时，机器学习模型仍然会犯许多错误。最近，Taori等人(2020)研究了图像分类模型的鲁棒性，而Miller等人(2020)研究了问题回答模型的鲁棒性。一个关键发现是多领域训练增加了鲁棒性和泛化性，正如在引言中所讨论的那样。这一发现在许多领域得到了复制，除了语音识别，还包括NLP(Hendrycks等人，2020)和计算机视觉(Radford等人，2021)。

## 6 限制和未来工作

从我们的实验结果、分析和割除中，我们注意到了一些限制和未来工作的领域。

**改进的解码策略。** 随着Whisper的扩展，我们观察到更大的模型在减少感知相关错误方面取得了稳定和可靠的进展，如混淆听起来相似的单词。许多剩余的错误，特别是在长篇转录中，似乎更加顽固，并且明显不是人类/感知性质的。它们是seq2seq模型、语言模型和文本-音频对齐的失败模式的组合，包括陷入重复循环、不转录音频段的第一个或最后几个词，或者完全幻觉，即模型会输出与实际音频完全无关的文本。尽管第4.5节中讨论的解码细节有很大帮助，但我们怀疑通过在高质量的监督数据集上对Whisper模型进行微调和/或使用强化学习来更直接地优化解码性能，可能有助于进一步减少这些错误。

**增加低资源语言的训练数据。** 正如图3所示，Whisper在许多语言上的语音识别性能仍然相当差。同样的分析表明了一个明确的改进途径，因为一个语言的性能很好地预测了该语言的训练数据量。由于我们的预训练数据集目前非常依赖英语，这是由于我们的数据收集管道的偏见，主要来自于英语中心的互联网部分，大多数语言的训练数据少于1000小时。有针对性地增加这些更稀缺语言的数据量，即使整体训练数据集的规模只有微小增加，也可能大大改善平均语音识别性能。

**研究微调** 在这项工作中，我们关注了语音处理系统的鲁棒性质，并且因此只研究了Whisper的零迁移性能。虽然这是一个重要的研究领域，因为它代表了一般的可靠性，但在许多领域中，存在高质量的监督语音数据，很可能通过微调进一步提高结果。研究微调的一个额外好处是，它允许与之前的工作进行直接比较，因为这是一个更常见的评估设置。

**研究语言模型对鲁棒性的影响** 正如在介绍中所论述的，我们怀疑Whisper的鲁棒性部分是由其强大的解码器造成的，这是一个音频条件的语言模型。目前尚不清楚Whisper的优势在多大程度上源自其编码器、解码器或两者的训练。可以通过消融Whisper的各种设计组件来研究这一点，例如训练一个无解码器的CTC模型，或者研究使用语言模型时，现有语音识别编码器的性能如何变化。

**添加辅助训练目标** Whisper与最近的最先进的语音识别系统有明显的不同之处，因为它缺乏无监督的预训练或自学习方法。虽然我们发现它们不是必需的来实现良好的性能，但通过将它们纳入可能进一步改善结果。

## 7 结论

Whisper表明，到目前为止，在语音识别研究中，弱监督预训练的扩展一直没有得到足够的重视。我们实现了我们的结果，而无需使用近期大规模语音识别工作中的自监督和自学习技术，并展示了简单地在大而多样的监督数据集上训练，并专注于零迁移如何显著提高语音识别系统的鲁棒性。





