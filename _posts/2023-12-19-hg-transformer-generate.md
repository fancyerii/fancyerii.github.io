---
layout:     post
title:      "Huggingface transformersæ–‡æœ¬ç”Ÿæˆ" 
author:     "lili" 
mathjax: true
sticky: false
excerpt_separator: <!--more-->
tags:
    - Huggingface
    - transformers 
    - text generation
---

æœ¬æ–‡æ•´ç†äº†[Huggingface transformers](https://github.com/huggingface/transformers)æ–‡æœ¬ç”Ÿæˆç›¸å…³çš„èµ„æ–™ã€‚

<!--more-->

**ç›®å½•**
* TOC
{:toc}


## æ–‡æœ¬ç”Ÿæˆç­–ç•¥
 
æ–‡æœ¬ç”Ÿæˆå¯¹äºè®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡è‡³å…³é‡è¦ï¼Œä¾‹å¦‚å¼€æ”¾å¼æ–‡æœ¬ç”Ÿæˆã€æ‘˜è¦ã€ç¿»è¯‘ç­‰ã€‚å®ƒè¿˜åœ¨è®¸å¤šæ··åˆæ¨¡æ€åº”ç”¨ä¸­å‘æŒ¥ä½œç”¨ï¼Œè¿™äº›åº”ç”¨ä»¥æ–‡æœ¬ä½œä¸ºè¾“å‡ºï¼Œå¦‚è¯­éŸ³è½¬æ–‡å­—å’Œè§†è§‰è½¬æ–‡å­—ã€‚ä¸€äº›èƒ½å¤Ÿç”Ÿæˆæ–‡æœ¬çš„æ¨¡å‹åŒ…æ‹¬GPT2ã€XLNetã€OpenAI GPTã€CTRLã€TransformerXLã€XLMã€Bartã€T5ã€GITã€Whisperã€‚


è¯·æ³¨æ„ï¼Œgenerateæ–¹æ³•çš„è¾“å…¥å–å†³äºæ¨¡å‹çš„æ¨¡æ€æ€§ã€‚è¿™äº›è¾“å…¥ç”±æ¨¡å‹çš„é¢„å¤„ç†å™¨ç±»ï¼ˆå¦‚AutoTokenizeræˆ–AutoProcessorï¼‰è¿”å›ã€‚å¦‚æœæ¨¡å‹çš„é¢„å¤„ç†å™¨åˆ›å»ºäº†å¤šç§ç±»å‹çš„è¾“å…¥ï¼Œè¯·å°†æ‰€æœ‰è¾“å…¥ä¼ é€’ç»™generate()ã€‚æ‚¨å¯ä»¥åœ¨ç›¸åº”æ¨¡å‹çš„æ–‡æ¡£ä¸­äº†è§£æœ‰å…³å„ä¸ªæ¨¡å‹é¢„å¤„ç†å™¨çš„æ›´å¤šä¿¡æ¯ã€‚

é€‰æ‹©ç”Ÿæˆæ–‡æœ¬çš„è¾“å‡ºtokençš„è¿‡ç¨‹ç§°ä¸ºè§£ç ï¼Œæ‚¨å¯ä»¥è‡ªå®šä¹‰generate()æ–¹æ³•å°†ä½¿ç”¨çš„è§£ç ç­–ç•¥ã€‚ä¿®æ”¹è§£ç ç­–ç•¥ä¸ä¼šæ›´æ”¹ä»»ä½•å¯è®­ç»ƒå‚æ•°çš„å€¼ã€‚ç„¶è€Œï¼Œå®ƒå¯èƒ½ä¼šæ˜¾è‘—å½±å“ç”Ÿæˆè¾“å‡ºçš„è´¨é‡ï¼Œæœ‰åŠ©äºå‡å°‘æ–‡æœ¬ä¸­çš„é‡å¤å¹¶ä½¿å…¶æ›´è¿è´¯ã€‚


### é»˜è®¤æ–‡æœ¬ç”Ÿæˆç­–ç•¥

æ¨¡å‹çš„è§£ç ç­–ç•¥æ˜¯åœ¨å…¶ç”Ÿæˆé…ç½®ä¸­å®šä¹‰çš„ã€‚åœ¨ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ç”¨pipelineè¿›è¡Œç”Ÿæˆæ—¶ï¼Œæ¨¡å‹è°ƒç”¨PreTrainedModel.generate()æ–¹æ³•ï¼Œåœ¨å¹•ååº”ç”¨é»˜è®¤çš„ç”Ÿæˆé…ç½®ã€‚å½“æ²¡æœ‰ä½¿ç”¨è‡ªå®šä¹‰é…ç½®ä¿å­˜æ¨¡å‹æ—¶ï¼Œä¹Ÿä¼šä½¿ç”¨é»˜è®¤é…ç½®ã€‚

å½“æ‚¨æ˜¾å¼åŠ è½½ä¸€ä¸ªæ¨¡å‹æ—¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡model.generation_configæ£€æŸ¥ç”Ÿæˆé…ç½®ï¼š

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("distilgpt2")
model.generation_config
è¾“å‡ºï¼š
GenerationConfig {
    "bos_token_id": 50256,
    "eos_token_id": 50256,
}
```
æ‰“å°model.generation_configåªæ˜¾ç¤ºä¸é»˜è®¤ç”Ÿæˆé…ç½®ä¸åŒçš„å€¼ï¼Œè€Œä¸åˆ—å‡ºä»»ä½•é»˜è®¤å€¼ã€‚

é»˜è®¤ç”Ÿæˆé…ç½®æ–°ç”Ÿæˆçš„æœ€å¤§tokenä¸º20ï¼Œä»¥é¿å…é‡åˆ°èµ„æºé™åˆ¶ã€è¿™å¯¹å¤§éƒ¨åˆ†åº”ç”¨æ¥è¯´æ˜¾ç„¶ä¸å¤Ÿã€‘ã€‚é»˜è®¤è§£ç ç­–ç•¥æ˜¯è´ªå©ªæœç´¢ï¼Œè¿™æ˜¯ä¸€ç§æœ€ç®€å•çš„è§£ç ç­–ç•¥ï¼Œé€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„tokenä½œä¸ºä¸‹ä¸€ä¸ªtokenã€‚å¯¹äºè®¸å¤šä»»åŠ¡å’Œå°çš„è¾“å‡ºå°ºå¯¸ï¼Œè¿™ç§æ–¹æ³•æ•ˆæœè‰¯å¥½ã€‚ç„¶è€Œï¼Œå½“ç”¨äºç”Ÿæˆè¾ƒé•¿çš„è¾“å‡ºæ—¶ï¼Œè´ªå©ªæœç´¢å¯èƒ½å¼€å§‹äº§ç”Ÿé«˜åº¦é‡å¤çš„ç»“æœã€‚

### å®šåˆ¶æ–‡æœ¬ç”Ÿæˆ

æ‚¨å¯ä»¥é€šè¿‡ç›´æ¥å°†å‚æ•°åŠå…¶å€¼ä¼ é€’ç»™generateæ–¹æ³•æ¥è¦†ç›–ä»»ä½•generation_configï¼š

```python
my_model.generate(**inputs, num_beams=4, do_sample=True)
```

å³ä½¿é»˜è®¤çš„è§£ç ç­–ç•¥å¯¹æ‚¨çš„ä»»åŠ¡å¤§å¤šæ•°æƒ…å†µä¸‹æœ‰æ•ˆï¼Œæ‚¨ä»ç„¶å¯ä»¥å¾®è°ƒä¸€äº›å‚æ•°ã€‚ä¸€äº›å¸¸å¸¸è°ƒæ•´çš„å‚æ•°åŒ…æ‹¬ï¼š

**max_new_tokens**ï¼šç”Ÿæˆçš„æœ€å¤§tokenæ•°ã€‚æ¢å¥è¯è¯´ï¼Œè¾“å‡ºåºåˆ—çš„å¤§å°ï¼Œä¸åŒ…æ‹¬æç¤ºä¸­çš„tokenã€‚ä½œä¸ºä½¿ç”¨è¾“å‡ºé•¿åº¦ä½œä¸ºåœæ­¢æ ‡å‡†çš„æ›¿ä»£æ–¹æ³•ï¼Œæ‚¨å¯ä»¥é€‰æ‹©åœ¨å®Œæ•´ç”Ÿæˆè¶…è¿‡ä¸€å®šæ—¶é—´é‡æ—¶åœæ­¢ç”Ÿæˆã€‚è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[StoppingCriteria](https://huggingface.co/docs/transformers/main/en/internal/generation_utils#transformers.StoppingCriteria)ã€‚

**num_beams**ï¼šé€šè¿‡æŒ‡å®šé«˜äº1çš„beamæ•°ï¼Œæ‚¨å®é™…ä¸Šæ˜¯ä»è´ªå©ªæœç´¢åˆ‡æ¢åˆ°beamæœç´¢ã€‚æ­¤ç­–ç•¥åœ¨æ¯ä¸ªæ—¶é—´æ­¥è¯„ä¼°å¤šä¸ªå‡è®¾ï¼Œæœ€ç»ˆé€‰æ‹©å…·æœ‰æ•´ä¸ªåºåˆ—æœ€é«˜æ¦‚ç‡çš„å‡è®¾ã€‚è¿™æœ‰åˆ©äºè¯†åˆ«ä»¥è¾ƒä½æ¦‚ç‡åˆå§‹tokenå¼€å¤´çš„é«˜æ¦‚ç‡åºåˆ—ï¼Œè€Œè¿™äº›åºåˆ—åœ¨è´ªå©ªæœç´¢ä¸­å¯èƒ½ä¼šè¢«å¿½ç•¥ã€‚

**do_sample**ï¼šå¦‚æœè®¾ç½®ä¸ºTrueï¼Œæ­¤å‚æ•°å¯ç”¨è§£ç ç­–ç•¥ï¼Œå¦‚å¤šé¡¹å¼é‡‡æ ·ã€beamæœç´¢å¤šé¡¹å¼é‡‡æ ·ã€Top-Ké‡‡æ ·å’ŒTop-pé‡‡æ ·ã€‚æ‰€æœ‰è¿™äº›ç­–ç•¥éƒ½ä»æ•´ä¸ªè¯æ±‡è¡¨çš„æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªtokenï¼Œå…·æœ‰å„ç§ç‰¹å®šäºç­–ç•¥çš„è°ƒæ•´ã€‚

**num_return_sequences**ï¼šæ¯ä¸ªè¾“å…¥è¿”å›çš„åºåˆ—å€™é€‰æ•°ã€‚æ­¤é€‰é¡¹ä»…é€‚ç”¨äºæ”¯æŒå¤šä¸ªåºåˆ—å€™é€‰çš„è§£ç ç­–ç•¥ï¼Œä¾‹å¦‚beamæœç´¢å’Œé‡‡æ ·çš„å˜ä½“ã€‚åƒè´ªå©ªæœç´¢å’Œå¯¹æ¯”æœç´¢è¿™æ ·çš„è§£ç ç­–ç•¥è¿”å›å•ä¸ªè¾“å‡ºåºåˆ—ã€‚

### ä¿å­˜è‡ªå®šä¹‰è§£ç ç­–ç•¥ä¸æ‚¨çš„æ¨¡å‹

å¦‚æœä½ è®­ç»ƒäº†è‡ªå·±çš„æ¨¡å‹ï¼Œå¹¶ä¸”æƒ³æŠŠé»˜è®¤çš„ç”Ÿæˆé…ç½®ä¿å­˜ä¸‹æ¥ï¼Œé‚£ä¹ˆå¯ä»¥æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

* åˆ›å»ºä¸€ä¸ªGenerationConfigç±»å®ä¾‹ã€‚
* æŒ‡å®šè§£ç ç­–ç•¥å‚æ•°ã€‚
* ä½¿ç”¨GenerationConfig.save_pretrained()ä¿å­˜æ‚¨çš„ç”Ÿæˆé…ç½®ï¼Œç¡®ä¿å°†å…¶config_file_nameå‚æ•°ç•™ç©ºã€‚
* å°†push_to_hubè®¾ç½®ä¸ºTrueï¼Œä»¥å°†æ‚¨çš„é…ç½®ä¸Šä¼ åˆ°æ¨¡å‹çš„ä»“åº“ã€‚

```python
from transformers import AutoModelForCausalLM, GenerationConfig

model = AutoModelForCausalLM.from_pretrained("my_account/my_model")
generation_config = GenerationConfig(
    max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id
)
generation_config.save_pretrained("my_account/my_model", push_to_hub=True)
```

æ‚¨è¿˜å¯ä»¥å°†å¤šä¸ªç”Ÿæˆé…ç½®å­˜å‚¨åœ¨å•ä¸ªç›®å½•ä¸­ï¼Œåˆ©ç”¨GenerationConfig.save_pretrained()ä¸­çš„config_file_nameå‚æ•°ã€‚ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨GenerationConfig.from_pretrained()å®ä¾‹åŒ–å®ƒä»¬ã€‚å¦‚æœæ‚¨å¸Œæœ›ä¸ºå•ä¸ªæ¨¡å‹å­˜å‚¨å¤šä¸ªç”Ÿæˆé…ç½®ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªç”¨äºé‡‡æ ·çš„åˆ›é€ æ€§æ–‡æœ¬ç”Ÿæˆï¼Œå¦ä¸€ä¸ªç”¨äºå¸¦æœ‰beamæœç´¢çš„æ‘˜è¦ï¼‰ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚æ‚¨å¿…é¡»å…·æœ‰é€‚å½“çš„Hubæƒé™æ‰èƒ½å°†é…ç½®æ–‡ä»¶æ·»åŠ åˆ°æ¨¡å‹ã€‚

```python
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("t5-small")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")

translation_generation_config = GenerationConfig(
    num_beams=4,
    early_stopping=True,
    decoder_start_token_id=0,
    eos_token_id=model.config.eos_token_id,
    pad_token=model.config.pad_token_id,
)

# Tip: add `push_to_hub=True` to push to the Hub
translation_generation_config.save_pretrained("/tmp", "translation_generation_config.json")

# You could then use the named generation config file to parameterize generation
generation_config = GenerationConfig.from_pretrained("/tmp", "translation_generation_config.json")
inputs = tokenizer("translate English to French: Configuration files are easy to use!", return_tensors="pt")
outputs = model.generate(**inputs, generation_config=generation_config)
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))
```

### Streaming

generate() æ”¯æŒæµå¼ä¼ è¾“ï¼Œé€šè¿‡å…¶streamerè¾“å…¥ã€‚streamerè¾“å…¥ä¸å…·æœ‰ä»¥ä¸‹æ–¹æ³•çš„ç±»çš„ä»»ä½•å®ä¾‹å…¼å®¹ï¼šput() å’Œ end()ã€‚åœ¨å†…éƒ¨ï¼Œput() ç”¨äºæ¨é€æ–°çš„tokenï¼Œè€Œend() ç”¨äºæ ‡è®°æ–‡æœ¬ç”Ÿæˆçš„ç»“æŸã€‚

>æµå¼ä¼ è¾“ç±»çš„ API ä»åœ¨å¼€å‘ä¸­ï¼Œæœªæ¥å¯èƒ½ä¼šå‘ç”Ÿæ›´æ”¹ã€‚

åœ¨å®è·µä¸­ï¼Œæ‚¨å¯ä»¥ä¸ºå„ç§ç›®çš„åˆ›å»ºè‡ªå·±çš„æµå¼ä¼ è¾“ç±»ï¼æˆ‘ä»¬è¿˜ä¸ºæ‚¨å‡†å¤‡äº†åŸºæœ¬çš„æµå¼ä¼ è¾“ç±»ä¾›æ‚¨ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨TextStreamerç±»å°†generate()çš„è¾“å‡ºæµå¼ä¼ è¾“åˆ°æ‚¨çš„å±å¹•ï¼Œé€è¯æ˜¾ç¤ºï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

tok = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")
inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
streamer = TextStreamer(tok)

# Despite returning the usual output, the streamer will also print the generated text to stdout.
_ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)
```


### è§£ç ç­–ç•¥

generate()å‚æ•°çš„æŸäº›ç»„åˆï¼Œæœ€ç»ˆä¼šä¿®æ”¹generation_configï¼Œå¯ä»¥ç”¨äºå¯ç”¨ç‰¹å®šçš„è§£ç ç­–ç•¥ã€‚å¦‚æœæ‚¨å¯¹è¿™ä¸ªæ¦‚å¿µä¸å¤ªäº†è§£ï¼Œæˆ‘ä»¬å»ºè®®é˜…è¯»[è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/how-to-generate)ã€åé¢ä¼šä»‹ç»è¿™ç¯‡åšå®¢ã€‘ï¼Œè¯¦ç»†è¯´æ˜äº†å¸¸è§è§£ç ç­–ç•¥çš„å·¥ä½œåŸç†ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å±•ç¤ºä¸€äº›æ§åˆ¶è§£ç ç­–ç•¥çš„å‚æ•°ï¼Œå¹¶è¯´æ˜æ‚¨å¦‚ä½•ä½¿ç”¨å®ƒä»¬ã€‚

####  è´ªå©ªæœç´¢(Greedy Search)

é»˜è®¤æƒ…å†µä¸‹ï¼Œgenerate ä½¿ç”¨è´ªå©ªæœç´¢è§£ç ï¼Œå› æ­¤æ‚¨æ— éœ€ä¼ é€’ä»»ä½•å‚æ•°æ¥å¯ç”¨å®ƒã€‚è¿™æ„å‘³ç€num_beamsè¢«è®¾ç½®ä¸º1ï¼Œè€Œdo_sampleè¢«è®¾ç½®ä¸ºFalseã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

prompt = "I look forward to"
checkpoint = "distilgpt2"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(checkpoint)
outputs = model.generate(**inputs)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

#### å¯¹æ¯”æœç´¢(Contrastive search)

å¯¹æ¯”æœç´¢è§£ç ç­–ç•¥æ˜¯åœ¨2022å¹´çš„è®ºæ–‡ã€Š[A Contrastive Framework for Neural Text Generation](https://arxiv.org/abs/2202.06417)ã€‹ä¸­æå‡ºçš„ã€‚è¯¥ç­–ç•¥åœ¨ç”Ÿæˆéé‡å¤ä½†è¿è´¯çš„é•¿æ–‡æœ¬è¾“å‡ºæ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„ç»“æœã€‚è¦äº†è§£å¯¹æ¯”æœç´¢çš„å·¥ä½œåŸç†ï¼Œè¯·æŸ¥é˜…[è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/introducing-csearch)ã€‚å¯ç”¨å’Œæ§åˆ¶å¯¹æ¯”æœç´¢è¡Œä¸ºçš„ä¸¤ä¸ªä¸»è¦å‚æ•°æ˜¯penalty_alphaå’Œtop_kï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

checkpoint = "gpt2-large"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)

prompt = "Hugging Face Company is"
inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_new_tokens=100)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

#### å¤šé¡¹å¼é‡‡æ ·(Multinomial sampling)

ä¸æ€»æ˜¯é€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„tokenä½œä¸ºä¸‹ä¸€ä¸ªtokençš„è´ªå©ªæœç´¢ç›¸åï¼Œå¤šé¡¹å¼é‡‡æ ·æ ¹æ®æ¨¡å‹ç»™å‡ºçš„æ•´ä¸ªè¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªtokenã€‚å…·æœ‰éé›¶æ¦‚ç‡çš„æ¯ä¸ªtokenéƒ½æœ‰è¢«é€‰æ‹©çš„æœºä¼šï¼Œä»è€Œé™ä½äº†é‡å¤çš„é£é™©ã€‚

è¦å¯ç”¨å¤šé¡¹å¼é‡‡æ ·ï¼Œè¯·è®¾ç½®do_sample=Trueå’Œnum_beams=1ã€‚

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
set_seed(0)  # For reproducibility

checkpoint = "gpt2-large"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)

prompt = "Today was an amazing day because"
inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

####  Beamæœç´¢è§£ç (Beam-search decoding)

ä¸è´ªå©ªæœç´¢ä¸åŒï¼Œbeamæœç´¢è§£ç åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¿ç•™å¤šä¸ªå‡è®¾ï¼Œæœ€ç»ˆé€‰æ‹©æ•´ä¸ªåºåˆ—ä¸­å…·æœ‰æœ€é«˜æ¦‚ç‡çš„å‡è®¾ã€‚è¿™æœ‰åˆ©äºè¯†åˆ«ä»¥è¾ƒä½æ¦‚ç‡åˆå§‹tokenå¼€å¤´çš„é«˜æ¦‚ç‡åºåˆ—ï¼Œè€Œè¿™äº›åºåˆ—åœ¨è´ªå©ªæœç´¢ä¸­å¯èƒ½ä¼šè¢«å¿½ç•¥ã€‚

è¦å¯ç”¨æ­¤è§£ç ç­–ç•¥ï¼Œè¯·æŒ‡å®šå¤§äº1çš„num_beamsã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

prompt = "It is astonishing how one can"
checkpoint = "gpt2-medium"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(checkpoint)

outputs = model.generate(**inputs, num_beams=5, max_new_tokens=50)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

#### Beamæœç´¢å¤šé¡¹å¼é‡‡æ ·(Beam-search multinomial sampling)

æ­£å¦‚å…¶åç§°æ‰€ç¤ºï¼Œè¿™ç§è§£ç ç­–ç•¥å°†beamæœç´¢ä¸å¤šé¡¹å¼é‡‡æ ·ç›¸ç»“åˆã€‚æ‚¨éœ€è¦æŒ‡å®šå¤§äº1çš„num_beamsï¼Œå¹¶è®¾ç½®do_sample=Trueä»¥ä½¿ç”¨æ­¤è§£ç ç­–ç•¥ã€‚

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, set_seed
set_seed(0)  # For reproducibility

prompt = "translate English to German: The house is wonderful."
checkpoint = "t5-small"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors="pt")

model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

outputs = model.generate(**inputs, num_beams=5, do_sample=True)
tokenizer.decode(outputs[0], skip_special_tokens=True)
```

<a>![](/img/moe/12.png)</a>
**MegaBlocks ä¸­æåˆ°çš„å—ç¨€ç–çŸ©é˜µä¹˜æ³•ï¼Œé€‚ç”¨äºä¸åŒå¤§å°çš„ä¸“å®¶å’Œæ ‡è®°æ•°é‡**

#### å¤šæ ·æ€§beamæœç´¢(Diverse beam search decoding)

å¤šæ ·æ€§beamæœç´¢è§£ç ç­–ç•¥æ˜¯beamæœç´¢ç­–ç•¥çš„æ‰©å±•ï¼Œå…è®¸ç”Ÿæˆä¸€ä¸ªæ›´å¤šæ ·åŒ–çš„beamåºåˆ—é›†åˆä¾›é€‰æ‹©ã€‚è¦äº†è§£å…¶å·¥ä½œåŸç†ï¼Œè¯·å‚é˜…ã€Š[Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424.pdf)ã€‹ã€‚è¯¥æ–¹æ³•æœ‰ä¸‰ä¸ªä¸»è¦å‚æ•°ï¼šnum_beamsã€num_beam_groupså’Œdiversity_penaltyã€‚å¤šæ ·æ€§æƒ©ç½šç¡®ä¿åœ¨ç»„é—´è¾“å‡ºæ˜¯ä¸åŒçš„ï¼Œè€Œåœ¨æ¯ä¸ªç»„å†…ä½¿ç”¨beamæœç´¢ã€‚


```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

checkpoint = "google/pegasus-xsum"
prompt = (
    "The Permaculture Design Principles are a set of universal design principles "
    "that can be applied to any location, climate and culture, and they allow us to design "
    "the most efficient and sustainable human habitation and food production systems. "
    "Permaculture is a design system that encompasses a wide variety of disciplines, such "
    "as ecology, landscape design, environmental science and energy conservation, and the "
    "Permaculture design principles are drawn from these various disciplines. Each individual "
    "design principle itself embodies a complete conceptual framework based on sound "
    "scientific principles. When we bring all these separate  principles together, we can "
    "create a design system that both looks at whole systems, the parts that these systems "
    "consist of, and how those parts interact with each other to create a complex, dynamic, "
    "living system. Each design principle serves as a tool that allows us to integrate all "
    "the separate parts of a design, referred to as elements, into a functional, synergistic, "
    "whole system, where the elements harmoniously interact and work together in the most "
    "efficient way possible."
)

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors="pt")

model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

outputs = model.generate(**inputs, num_beams=5, num_beam_groups=5, max_new_tokens=30, diversity_penalty=1.0)
tokenizer.decode(outputs[0], skip_special_tokens=True)
```

æœ¬æŒ‡å—é˜æ˜äº†å¯ç”¨å„ç§è§£ç ç­–ç•¥çš„ä¸»è¦å‚æ•°ã€‚generateæ–¹æ³•è¿˜å­˜åœ¨æ›´é«˜çº§çš„å‚æ•°ï¼Œå®ƒä»¬è®©æ‚¨å¯¹generateæ–¹æ³•çš„è¡Œä¸ºæœ‰æ›´è¿›ä¸€æ­¥çš„æ§åˆ¶ã€‚æœ‰å…³æ‰€æœ‰å¯ç”¨å‚æ•°çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·å‚é˜…APIæ–‡æ¡£ã€‚

#### è¾…åŠ©è§£ç (Assisted Decoding)

è¾…åŠ©è§£ç æ˜¯å¯¹ä¸Šè¿°è§£ç ç­–ç•¥çš„ä¿®æ”¹ï¼Œå®ƒä½¿ç”¨å…·æœ‰ç›¸åŒåˆ†è¯å™¨çš„è¾…åŠ©æ¨¡å‹æ¨¡å‹ï¼ˆç†æƒ³æƒ…å†µä¸‹æ˜¯ä¸€ä¸ªæ›´å°çš„æ¨¡å‹ï¼‰è´ªå©ªç”Ÿæˆå‡ ä¸ªå€™é€‰tokenã€‚ç„¶åï¼Œä¸»æ¨¡å‹åœ¨å•ä¸ªå‰å‘ä¼ é€’ä¸­éªŒè¯å€™é€‰tokenï¼Œä»è€ŒåŠ é€Ÿè§£ç è¿‡ç¨‹ã€‚ç›®å‰ï¼Œè¾…åŠ©è§£ç ä»…æ”¯æŒè´ªå©ªæœç´¢å’Œé‡‡æ ·ï¼Œå¹¶ä¸”ä¸æ”¯æŒæ‰¹å¤„ç†è¾“å…¥ã€‚è¦äº†è§£æœ‰å…³è¾…åŠ©è§£ç çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥é˜…[æ­¤åšå®¢æ–‡ç« ](https://huggingface.co/blog/assisted-generation)ã€‚

è¦å¯ç”¨è¾…åŠ©è§£ç ï¼Œè¯·ä½¿ç”¨assistant_modelå‚æ•°è®¾ç½®ä¸€ä¸ªæ¨¡å‹ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

prompt = "Alice and Bob"
checkpoint = "EleutherAI/pythia-1.4b-deduped"
assistant_checkpoint = "EleutherAI/pythia-160m-deduped"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
inputs = tokenizer(prompt, return_tensors="pt")

model = AutoModelForCausalLM.from_pretrained(checkpoint)
assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
outputs = model.generate(**inputs, assistant_model=assistant_model)
tokenizer.batch_decode(outputs, skip_special_tokens=True)
```

#### ç›¸å…³ä»£ç 

ä¸Šé¢çš„è§£ç ç­–ç•¥é€‰æ‹©ï¼Œå¯ä»¥é€šè¿‡[transformers/generation/utils.py](https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py#L968)é‡Œçš„ä»£ç æ¥éªŒè¯ï¼š

```python
    def _get_generation_mode(
        self, generation_config: GenerationConfig, assistant_model: Optional["PreTrainedModel"]
    ) -> GenerationMode:
        """
        Returns the generation mode triggered by a [`GenerationConfig`] instance.
        """
        if generation_config.constraints is not None or generation_config.force_words_ids is not None:
            generation_mode = GenerationMode.CONSTRAINED_BEAM_SEARCH
        elif generation_config.num_beams == 1:
            if generation_config.do_sample is False:
                if (
                    generation_config.top_k is not None
                    and generation_config.top_k > 1
                    and generation_config.penalty_alpha is not None
                    and generation_config.penalty_alpha > 0
                ):
                    generation_mode = GenerationMode.CONTRASTIVE_SEARCH
                else:
                    generation_mode = GenerationMode.GREEDY_SEARCH
            else:
                generation_mode = GenerationMode.SAMPLE
        else:
            if generation_config.num_beam_groups > 1:
                generation_mode = GenerationMode.GROUP_BEAM_SEARCH
            elif generation_config.do_sample is True:
                generation_mode = GenerationMode.BEAM_SAMPLE
            else:
                generation_mode = GenerationMode.BEAM_SEARCH

        # Assisted generation may extend some generation modes
        if assistant_model is not None:
            if generation_mode in ("greedy_search", "sample"):
                generation_mode = GenerationMode.ASSISTED_GENERATION
            else:
                raise ValueError(
                    "You've set `assistant_model`, which triggers assisted generate. Currently, assisted generate "
                    "is only supported with Greedy Search and Sample."
                )
        return generation_mode
```

##### 1. çº¦æŸçš„Beamæœç´¢(Constrained Beam Search)

å¦‚æœconstraintsæˆ–è€…force_words_idséç©ºï¼Œå‡½æ•°è¿”å›CONSTRAINED_BEAM_SEARCHã€‚çº¦æŸçš„Beamæœç´¢å¯ä»¥å¼ºåˆ¶è§£ç çš„ç»“æœåŒ…å«æŸäº›tokenæˆ–è€…æ›´å¤æ‚çš„çº¦æŸ(å¦‚çŸ­è¯­çº¦æŸ)ï¼Œè¯¦ç»†ä»‹ç»å¯ä»¥å‚è€ƒ[Guiding Text Generation with Constrained Beam Search in ğŸ¤— Transformers](https://huggingface.co/blog/constrained-beam-search)ã€‚

##### 2. è´ªå©ªæœç´¢

num_beams==1å¹¶ä¸”do_sample==Falseæ—¶æ‰§è¡Œè¿™ä¸ªç®—æ³•ã€‚è´ªå©ªç®—æ³•åœ¨æ¯ä¸ªæ—¶åˆ»éƒ½é€‰æ‹©æ¦‚ç‡(logit)æœ€å¤§çš„é‚£ä¸ªtokenï¼Œè¿™æ˜¯æœ€ç®€å•çš„ç®—æ³•ã€‚

<a>![](/img/textgen/1.png)</a>

æ¯”å¦‚ä¸Šå›¾ï¼Œpromptæ˜¯"The"ï¼Œç„¶åç¬¬ä¸€ä¸ªæ—¶åˆ»æ¦‚ç‡æœ€å¤§çš„æ˜¯"nice"ã€"dog"å’Œ"car"ï¼Œè´ªå©ªç®—æ³•é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„niceã€‚åé¢ä¹Ÿæ˜¯ç±»ä¼¼çš„ã€‚æ‰€ä»¥è´ªå©ªç®—æ³•æ˜¯ä¸€ç§ç¡®å®šæ€§çš„ç®—æ³•ã€‚

##### 3. å¯¹æ¯”æœç´¢

penalty_alpha>0å¹¶ä¸”top_k>1æ—¶æ‰§è¡Œè¿™ä¸ªç®—æ³•ã€‚è¿™ä¸ªç®—æ³•è¯¦ç»†ä»‹ç»è¯·å‚è€ƒ[Generating Human-level Text with Contrastive Search in Transformers](https://huggingface.co/blog/introducing-csearch)ï¼Œè¿™é‡Œç®€å•ä»‹ç»ä¸€ä¸‹å®ƒçš„æ€æƒ³ã€‚è¿™ä¸ªç®—æ³•çš„è§£ç ç®—æ³•å¦‚ä¸‹å¼å­æ‰€ç¤ºï¼š

<a>![](/img/textgen/2.png)</a>

å…¶ä¸­ï¼Œ$V^{(k)}$ æ˜¯è¯­è¨€æ¨¡å‹æ¦‚ç‡åˆ†å¸ƒ$p_\theta(v \| x_{<t})$çš„æ¦‚ç‡æ’åœ¨top-kçš„kä¸ªtokenã€‚ç¬¬ä¸€é¡¹ï¼Œå³æ¨¡å‹ç½®ä¿¡åº¦ï¼Œæ˜¯è¯­è¨€æ¨¡å‹å¯¹å€™é€‰ v çš„é¢„æµ‹æ¦‚ç‡ã€‚ç¬¬äºŒé¡¹ï¼Œé€€åŒ–æƒ©ç½šï¼Œè¡¨ç¤ºäº†å€™é€‰ v ç›¸å¯¹äºå…ˆå‰ä¸Šä¸‹æ–‡$x_{<t}$å’Œå‡½æ•°$s(â‹…,â‹…)$è®¡ç®—çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚å…·ä½“è€Œè¨€ï¼Œé€€åŒ–æƒ©ç½šå®šä¹‰ä¸ºå€™é€‰ v çš„tokençš„éšå‘é‡$h_v$ ä¸å…ˆå‰ä¸Šä¸‹æ–‡ $x_{<t}$ä¸­æ‰€æœ‰tokençš„éšå‘é‡ä¹‹é—´çš„æœ€å¤§ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ç›´è§‚åœ°è¯´ï¼Œå€™é€‰ v çš„è¾ƒå¤§é€€åŒ–æƒ©ç½šæ„å‘³ç€å®ƒåœ¨è¡¨ç¤ºç©ºé—´ä¸­æ›´ç±»ä¼¼äºä¸Šä¸‹æ–‡ï¼Œå› æ­¤æ›´æœ‰å¯èƒ½å¯¼è‡´æ¨¡å‹é€€åŒ–é—®é¢˜ã€‚è¶…å‚æ•°$\alpha$è°ƒèŠ‚è¿™ä¸¤ä¸ªç»„æˆéƒ¨åˆ†çš„é‡è¦æ€§ã€‚å½“$\alpha=0$æ—¶ï¼Œå¯¹æ¯”æœç´¢å˜ä¸ºåŸºæœ¬çš„è´ªå©ªæœç´¢ã€‚

##### 4. å¤šé¡¹å¼é‡‡æ ·

num_beams==1å¹¶ä¸”do_sample==Trueæ—¶æ‰§è¡Œã€‚æ¯æ¬¡æ ¹æ®softmax(logits)çš„å¤šé¡¹å¼æ¦‚ç‡è¿›è¡Œé‡‡æ ·ã€‚ä¸ºäº†é˜²æ­¢æ¦‚ç‡ç‰¹åˆ«ä½çš„è¯è¢«é‡‡æ ·ï¼Œå¯ä»¥ä½¿ç”¨top_pæˆ–è€…top_kè¿›è¡Œè¿‡æ»¤ã€‚

top_kæ¯”è¾ƒç®€å•ï¼Œåªä¿ç•™æ¦‚ç‡æœ€å¤§çš„kä¸ªtokenï¼Œç„¶åé‡æ–°ç”¨softmaxè®¡ç®—å…¶æ¦‚ç‡ã€‚ä½†æ˜¯è¿™æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºtop_kæ˜¯å›ºå®šçš„ï¼Œæœ‰çš„æ—¶å€™ï¼ŒæŸäº›tokenå…¶å®æ¦‚ç‡å¾ˆä½ï¼Œä½†æ˜¯å› ä¸€å®šè¦å‡‘å¤Ÿkä¸ªï¼Œä¹Ÿä¼šè¢«ç•™ä¸‹æ¥ã€‚ä¸ä¹‹ç›¸åï¼Œæœ‰çš„æ—¶å€™ï¼Œç”±äºæ¦‚ç‡åˆ†å¸ƒå¾ˆå‡åŒ€ï¼Œæ’åœ¨top_k+1çš„tokenä¹Ÿå¯èƒ½æ¦‚ç‡è¿˜ä¸å°ï¼Œä½†æ˜¯ä¸èƒ½ç•™ä¸‹æ¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œtop_pè¿™ä¸ªå‚æ•°æ´¾ä¸Šç”¨åœºï¼Œæ¯”å¦‚top_p=0.92ï¼Œé‚£ä¹ˆåªæœ‰æŸä¸ªtokençš„æ¦‚ç‡æ˜¯æœ€å¤§æ¦‚ç‡tokençš„92%ä»¥ä¸Šéƒ½ä¼šè¢«ä¿ç•™ä¸‹æ¥ã€‚è¿™ä¸¤ä¸ªå‚æ•°å¯ä»¥åŒæ—¶ä½¿ç”¨ï¼Œé‚£ä¹ˆå°±æ˜¯ä¸€ç§é€»è¾‘ä¸çš„å…³ç³»ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªæ¡ä»¶ä¸æ»¡è¶³å°±ä¼šè¢«æ‰”æ‰ã€‚

é™¤äº†è¿™ä¸¤ä¸ªå‚æ•°ï¼Œè¿˜æœ‰ä¸€ä¸ªtemperatureå‚æ•°ï¼Œè¿™ä¸ªå‚æ•°è¶Šå°ï¼Œé‚£ä¹ˆé‡‡æ ·è¶Šå€¾å‘äºæ¦‚ç‡å¤§çš„tokenã€‚æç«¯æƒ…å†µå¦‚æœtemperatureè¶‹è¿‘äºé›¶(è¿™é‡Œä¸èƒ½ä¸ºé›¶ï¼Œä½†æ˜¯åƒvLLMå¯ä»¥è®¾ç½®ä¸ºé›¶)ï¼Œå®ƒå°±ç­‰ä»·äºè´ªå¿ƒç®—æ³•ã€‚

##### 5. Beamæœç´¢

num_beams>1å¹¶ä¸”`do_sample=False`ã€‚è´ªå¿ƒç®—æ³•çš„é—®é¢˜æ˜¯æœ‰çš„æ—¶å€™å‰é¢æŸä¸ªè¯çš„æ¦‚ç‡æŒºé«˜ï¼Œä½†æ˜¯åé¢å°±æ²¡æœ‰å¥½çš„tokenäº†ã€‚å°±åƒäººç”Ÿï¼Œå¼€å±€å¥½ä¸è§å¾—å…¨å±€å¥½ã€‚è´ªå¿ƒç®—æ³•ä¼šæŠŠä¸€äº›å…¨å±€è¾ƒä¼˜ä½†æ˜¯ä¸€å¼€å§‹ä¸å¥½çš„è·¯å¾„æ·˜æ±°æ‰ï¼Œä¸ºäº†é¿å…è¿™ä¸ªé—®é¢˜ï¼ŒBeamæœç´¢ä¼šåœ¨æ¯ä¸ªæ—¶åˆ»åŒæ—¶ä¿ç•™æœ€ä¼˜çš„num_beamsæ¡è·¯å¾„ã€‚å½“ç„¶ä½ è¯´ä¸€ä¸Šæ¥çš„æ’åå€’æ•°ç¬¬ä¸€ï¼Œåé¢é€†è¢­æˆä¸ºç¬¬ä¸€ï¼Œè¿™ç§äº‹æƒ…ä¹Ÿä¸æ˜¯æ²¡æœ‰ï¼Œä½†æ˜¯æ¦‚ç‡å¤ªå°äº†ï¼Œè€Œä¸”ä»è®¡ç®—çš„è§’åº¦æ¥è¯´ä¿ç•™æ‰€æœ‰è·¯å¾„åŸºæœ¬æ˜¯ä¸å¯èƒ½çš„ã€‚Beamæœç´¢çš„ä¸€ä¸ªç¤ºä¾‹å¦‚ä¸‹å›¾ï¼š

<a>![](/img/textgen/3.png)</a>

è¿™ä¸ªç¤ºä¾‹beamä¸º2ï¼Œç¬¬ä¸€æ¬¡é€‰æ‹©æ—¶æ’åœ¨ç¬¬2çš„"dog"ä¹Ÿè¢«ä¿ç•™ä¸‹æ¥ã€‚æœ€åå…¨å±€æœ€ä¼˜çš„æ˜¯å¾—åˆ†0.9çš„è·¯å¾„æˆä¸ºæœ€ç»ˆç»“æœã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒBeamæœç´¢ä¹Ÿæ˜¯ç¡®å®šæ€§çš„ç®—æ³•ï¼Œå®ƒç›¸æ¯”è´ªå¿ƒç®—æ³•æ›´å¯èƒ½æ‰¾åˆ°å…¨å±€æœ€ä¼˜ï¼Œä½†æ˜¯ç”±äºæ¯æ¬¡ä¿ç•™å’Œå±•å¼€å¤šæ¡è·¯å¾„ï¼Œå› æ­¤å…¶é€Ÿåº¦æ¯”è´ªå¿ƒç®—æ³•æ…¢ã€‚num_beamsè¶Šå¤§ï¼Œé€Ÿåº¦è¶Šæ…¢ï¼Œæ‰¾åˆ°å…¨å±€æœ€ä¼˜çš„å¯èƒ½æ€§è¶Šå¤§ã€‚


##### 6. å¤šæ ·æ€§Beamæœç´¢(Group Beam Search)

`æ¡ä»¶ä¸ºnum_beams>1å¹¶ä¸”num_beam_groups>1ã€‚å¤šæ ·æ€§Beamæœç´¢ä»»åŠ¡æ™®é€šçš„Beamæœç´¢è™½ç„¶ä¼šä¿ç•™å¤šæ¡è·¯å¾„ï¼Œä½†æ˜¯è¿™äº›è·¯å¾„çš„ç›¸ä¼¼åº¦éƒ½å¾ˆé«˜(æ¯”å¦‚"the book is mine"æŠŠmineæ”¹æˆhisã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¤šæ ·æ€§beamæœç´¢æŠŠnum_beamsåˆ†æˆnum_beam_groupsä¸ªç»„ï¼Œæ¯ä¸ªç»„num_beams/num_beam_groupsæ¡è·¯å¾„ï¼Œæ¯ä¸ªç»„ä½¿ç”¨æ™®é€šçš„beamæœç´¢è§£ç ã€‚ä½†æ˜¯ä»ç¬¬äºŒä¸ªç»„å¼€å§‹ï¼Œé™¤äº†ç”¨LLMè®¡ç®—ç”Ÿæˆtokençš„æ¦‚ç‡ï¼Œè¿˜ä¼šå¢åŠ ä¸€ä¸ªæ‰€è°“çš„ä¸ç›¸ä¼¼é¡¹ï¼š

<a>![](/img/textgen/4.png)</a>

è¿™ä¸ªå…¬å¼ç”¨è‡ªç„¶è¯­è¨€æè¿°å°±æ˜¯ï¼šå®ƒå®šä¹‰äº†ä¸€ä¸ªåºåˆ—$y_{[t]}$å’Œç¬¬gç»„çš„æ‰€æœ‰åºåˆ—$Y_{[t]}^g$çš„ä¸ç›¸ä¼¼åº¦ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªè¶Šå¤§è¶Šå¥½ï¼Œè¿™æ ·å°±èƒ½æœ‰å¤šæ ·æ€§ã€‚

å› æ­¤ç¬¬gç»„çš„beamæœç´¢ç›®æ ‡å˜æˆï¼š

<a>![](/img/textgen/5.png)</a>

çœ‹èµ·æ¥è¿™ä¸ªå…¬å¼å¾ˆå¤æ‚ï¼Œä»”ç»†åˆ†æå…¶å®å®ƒè¡¨è¾¾çš„å°±æ˜¯ï¼šé™¤äº†LLMçš„ç”Ÿæˆæ¦‚ç‡(ç¬¬ä¸€é¡¹)ï¼Œè¿˜éœ€è¦è€ƒè™‘ç¬¬äºŒé¡¹é‚£ä¸ªsumæ±‚å’Œï¼Œå®ƒè®¡ç®—å½“å‰è§£ç åºåˆ—$y^g_{b,[t]}$å’Œä¹‹å‰çš„1ï½g-1ä¸ªç»„çš„ä¸ç›¸ä¼¼åº¦ã€‚

æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼š

<a>![](/img/textgen/6.png)</a>

æ¯”å¦‚åœ¨ç¬¬4ä¸ªæ—¶åˆ»ï¼Œè§£ç gruop3çš„æ—¶å€™ï¼Œ"several birds are"ä¼šé¿å…ä¹‹å‰groupå‡ºç°è¿‡çš„"birds, the , an"è¿™äº›è¯ã€‚å¤§æ¦‚åŸç†å°±æ˜¯è¿™æ ·ï¼Œæ›´å¤šç»†èŠ‚è¯·å‚è€ƒè®ºæ–‡ã€‚ä½¿ç”¨çš„æ–¹æ³•ï¼Œé™¤äº†å‚æ•°num_beamså’Œnum_beam_groupsï¼Œç»†å¿ƒçš„è¯»è€…å¯èƒ½ä¼šé—®æ€ä¹ˆæ§åˆ¶è¶…å‚æ•°$\lambda_g$å‘¢ï¼Ÿè¿™ä¸ªå‚æ•°è¶Šå¤§ï¼Œé‚£ä¹ˆç»„ä¹‹é—´çš„å·®å¼‚è¶Šå¤§ã€‚å¦å¤–ä¸ç›¸ä¼¼åº¦æ˜¯æ€ä¹ˆå®šä¹‰çš„å‘¢ï¼ŸHuggingfaceå®ç°çš„æ˜¯[HammingDiversityLogitsProcessor](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.HammingDiversityLogitsProcessor)ï¼Œå…·ä½“åŸç†ä¸ä»‹ç»äº†ã€‚æ„Ÿå…´è¶£çš„è¯»è€…è‡ªå·±çœ‹è®ºæ–‡å’Œé˜…è¯»ä»£ç å§ã€‚ç”¨æ³•å°±æ˜¯é€šè¿‡å‚æ•°diversity_penaltyæ§åˆ¶ï¼š

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# Initialize the model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# A long text about the solar system
text = (
    "The Solar System is a gravitationally bound system comprising the Sun and the objects that orbit it, "
    "either directly or indirectly. Of the objects that orbit the Sun directly, the largest are the eight "
    "planets, with the remainder being smaller objects, such as the five dwarf planets and small Solar System "
    "bodies. The Solar System formed 4.6 billion years ago from the gravitational collapse of a giant "
    "interstellar molecular cloud."
)
inputs = tokenizer("summarize: " + text, return_tensors="pt")

# Generate diverse summary
outputs_diverse = model.generate(
    **inputs,
    num_beam_groups=2,
    diversity_penalty=10.0,
    max_length=100,
    num_beams=4,
    num_return_sequences=2,
)
summaries_diverse = tokenizer.batch_decode(outputs_diverse, skip_special_tokens=True)

# Generate non-diverse summary
outputs_non_diverse = model.generate(
    **inputs,
    max_length=100,
    num_beams=4,
    num_return_sequences=2,
)
summary_non_diverse = tokenizer.batch_decode(outputs_non_diverse, skip_special_tokens=True)

# With `diversity_penalty`, the resulting beams are much more diverse
print(summary_non_diverse)

print(summaries_diverse)
```

å‚æ•°diversity_penaltyæœ€ç»ˆä¼šä¼ ç»™transformers.HammingDiversityLogitsProcessorã€‚

##### 7. å¤šé¡¹å¼é‡‡æ ·Beamæœç´¢

éœ€è¦æ»¡è¶³çš„æ¡ä»¶æ˜¯num_beams>1å’Œdo_sample=Trueã€‚è¿™æ˜¯ChatGPTå’ŒGPT-4ä½¿ç”¨çš„æœç´¢æ–¹æ³•ã€‚æœ‰äººè®¤ä¸ºChatGPTä½¿ç”¨äº†å¤šé¡¹å¼é‡‡æ ·çš„Beamæœç´¢ï¼Œæ¯”å¦‚[reddit: chatGPT uses beam search](https://www.reddit.com/r/LocalLLaMA/comments/13zlbt6/chatgpt_uses_beam_search_your_local_models_use/)ã€‚æˆ‘è®¤ä¸ºä¸å¤ªå¯èƒ½ï¼ŒåŸå› æœ‰äºŒã€‚ä¸€æ˜¯åœ¨è®ºæ–‡ä¸­InstructGPTé‡Œæåˆ°è¿‡ç”¨çš„æ˜¯top_pé‡‡æ ·ï¼Œredditå›å¸–ä¸­æœ‰äººæœ‰æåˆ°è¿‡ã€‚ç¬¬äºŒä¸ªåŸå› æ˜¯beamæœç´¢æ˜¯æ— æ³•å®ç°æµå¼(streaming)è¾“å‡ºçš„ï¼Œå› ä¸ºå®ƒæ¯æ¬¡ä¿ç•™å¤šä¸ªç»“æœï¼Œä¹‹å‰è¾ƒä¼˜çš„ç»“æœå¯èƒ½åˆ°æœ€åå°±ä¸æ˜¯æœ€ä¼˜äº†ã€‚å¾ˆå¤šå®æ—¶è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¹Ÿä¼šæœ‰æµå¼è§£ç ï¼Œä½†æ˜¯æˆ‘ä»¬ä»”ç»†è§‚å¯Ÿä¼šå‘ç°æœ€ç»ˆç»“æœçš„å‰ç¼€å¯èƒ½å¹¶ä¸æ˜¯ä¹‹å‰çš„éƒ¨åˆ†ç»“æœã€‚ä½†æ˜¯ChatGPTç¬¬ä¸€ä¸ªå­—è¾“å‡ºåæ˜¯æ°¸è¿œä¸ä¼šæ”¹å˜çš„ã€‚å½“ç„¶ç†è®ºä¸Šä¹Ÿå¯èƒ½ç”¨beamæœç´¢è§£ç Nä¸ªå­—ï¼Œç„¶åä¸€æ¬¡è¾“å‡ºï¼Œç„¶ååŸºäºå®ƒ(å‰Nä¸ªå­—ä¸å˜)å†è¿›è¡Œbeamæœç´¢ã€‚ä¸è¿‡æˆ‘è®¤ä¸ºè¿™ç§å¯èƒ½æ€§ä¸å¤§ã€‚

beamæœç´¢æœ€ç®€å•å°±æ˜¯top_kï¼Œå½“ç„¶ä¹Ÿå¯ä»¥ç”¨top_pã€‚å¦å¤–å°±æ˜¯temperatureå‚æ•°ï¼Œå¦‚æœtemperatureè¶‹è¿‘äº0ï¼Œåˆ™beamæœç´¢æ¥è¿‘äºè´ªå¿ƒç®—æ³•ã€‚

##### 8. è¾…åŠ©æœç´¢

è¿™ä¸ªæœç´¢å’Œå‰é¢ä¸ä¸€æ ·ï¼Œå®ƒä¸èƒ½å•ç‹¬è¿è¡Œï¼Œè€Œæ˜¯ä½œä¸ºå¦å¤–ä¸€ä¸ªæ¨¡å‹çš„è¾…åŠ©ã€‚è¯¦ç»†åŸç†å‚è€ƒ[Assisted Generation: a new direction toward low-latency text generation](https://huggingface.co/blog/assisted-generation)ã€‚è¿™é‡Œåªåšç®€å•çš„ä»‹ç»ã€‚

###### 8.1 è¯­è¨€æ¨¡å‹è§£ç å›é¡¾

åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå…¸å‹çš„è¿­ä»£åŒ…æ‹¬æ¨¡å‹æ¥æ”¶æœ€æ–°ç”Ÿæˆçš„tokenä½œä¸ºè¾“å…¥ï¼Œå†åŠ ä¸Šæ‰€æœ‰å…¶ä»–å…ˆå‰è¾“å…¥çš„ç¼“å­˜å†…éƒ¨è®¡ç®—ï¼Œç„¶åè¿”å›ä¸‹ä¸€ä¸ªtokençš„logitsã€‚ç¼“å­˜ç”¨äºé¿å…å†—ä½™è®¡ç®—ï¼Œä»è€ŒåŠ é€Ÿå‰å‘ä¼ é€’ï¼Œä½†è¿™å¹¶éå¼ºåˆ¶æ€§ï¼ˆå¯ä»¥éƒ¨åˆ†ä½¿ç”¨ï¼‰ã€å…³äºKV cacheè¯»è€…å¯ä»¥å‚è€ƒ[PagedAttentionè®ºæ–‡è§£è¯»](/2023/11/01/pagedattention)ã€‘ã€‚å½“ç¦ç”¨ç¼“å­˜æ—¶ï¼Œè¾“å…¥åŒ…å«åˆ°ç›®å‰ä¸ºæ­¢ç”Ÿæˆçš„æ‰€æœ‰tokenåºåˆ—ï¼Œè¾“å‡ºåŒ…å«ä¸åºåˆ—ä¸­æ‰€æœ‰ä½ç½®å¯¹åº”çš„ä¸‹ä¸€ä¸ªtokençš„logitsï¼åœ¨ä½ç½®Nå¤„çš„logitså¯¹åº”äºå¦‚æœè¾“å…¥åŒ…å«å‰Nä¸ªtokenï¼Œåˆ™ä¸‹ä¸€ä¸ªtokençš„åˆ†å¸ƒï¼Œå¿½ç•¥åºåˆ—ä¸­çš„æ‰€æœ‰åç»­tokenã€‚åœ¨è´ªå©ªè§£ç çš„ç‰¹å®šæƒ…å†µä¸‹ï¼Œå¦‚æœå°†ç”Ÿæˆçš„åºåˆ—ä½œä¸ºè¾“å…¥ä¼ é€’ï¼Œå¹¶å¯¹ç”Ÿæˆçš„logitsåº”ç”¨argmaxè¿ç®—ç¬¦ï¼Œæ‚¨å°†è·å¾—ç”Ÿæˆçš„åºåˆ—ã€‚


```python
from transformers import AutoModelForCausalLM, AutoTokenizer

tok = AutoTokenizer.from_pretrained("distilgpt2")
model = AutoModelForCausalLM.from_pretrained("distilgpt2")

inputs = tok(["The"], return_tensors="pt")
generated = model.generate(**inputs, do_sample=False, max_new_tokens=10)
forward_confirmation = model(generated).logits.argmax(-1)

# We exclude the opposing tips from each sequence: the forward pass returns
# the logits for the next token, so it is shifted by one position.
print(generated[0, 1:].tolist() == forward_confirmation[0, :-1].tolist())  # True
```

è¿™æ„å‘³ç€æ‚¨å¯ä»¥å°†æ¨¡å‹çš„å‰å‘ä¼ é€’ç”¨äºä¸åŒçš„ç›®çš„ï¼šé™¤äº†æä¾›ä¸€äº›tokenä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªtokenä¹‹å¤–ï¼Œè¿˜å¯ä»¥å°†ä¸€ä¸ªåºåˆ—ä¼ é€’ç»™æ¨¡å‹ï¼Œç„¶ååŒé‡æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä¼šç”Ÿæˆç›¸åŒçš„åºåˆ—ï¼ˆæˆ–å…¶ä¸­çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚

###### 8.2 ä½¿ç”¨è¾…åŠ©æœç´¢çš„è´ªå¿ƒè§£ç ç®—æ³•

æ‚¨å¸Œæœ›è¾…åŠ©æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿç”Ÿæˆå€™é€‰åºåˆ—ï¼ŒåŒæ—¶å°½å¯èƒ½å‡†ç¡®ã€‚å¦‚æœè¾…åŠ©æ¨¡å‹çš„è´¨é‡è¾ƒå·®ï¼Œæ‚¨å°†ä»˜å‡ºä½¿ç”¨è¾…åŠ©æ¨¡å‹æ¨¡å‹çš„æˆæœ¬ï¼Œå´å‡ ä¹æ²¡æœ‰ä»»ä½•å¥½å¤„ã€‚å¦ä¸€æ–¹é¢ï¼Œä¼˜åŒ–å€™é€‰åºåˆ—çš„è´¨é‡å¯èƒ½æ„å‘³ç€ä½¿ç”¨è¾ƒæ…¢çš„è¾…åŠ©æ¨¡å‹ï¼Œå¯¼è‡´å‡€å‡é€Ÿã€‚è™½ç„¶æˆ‘ä»¬ä¸èƒ½ä¸ºæ‚¨è‡ªåŠ¨é€‰æ‹©è¾…åŠ©æ¨¡å‹æ¨¡å‹ï¼Œä½†æˆ‘ä»¬å·²ç»åŒ…å«äº†ä¸€ä¸ªé¢å¤–çš„è¦æ±‚å’Œä¸€ä¸ªå¯å‘å¼æ–¹æ³•ï¼Œä»¥ç¡®ä¿ä¸è¾…åŠ©æ¨¡å‹ä¸€èµ·èŠ±è´¹çš„æ—¶é—´å¾—åˆ°æ§åˆ¶ã€‚

é¦–å…ˆï¼Œè¦æ±‚è¾…åŠ©æ¨¡å‹å¿…é¡»å…·æœ‰ä¸æ‚¨çš„æ¨¡å‹å®Œå…¨ç›¸åŒçš„åˆ†è¯å™¨ã€‚å¦‚æœæ²¡æœ‰è¿™ä¸ªè¦æ±‚ï¼Œå°±å¿…é¡»æ·»åŠ æ˜‚è´µçš„tokenè§£ç å’Œé‡æ–°ç¼–ç æ­¥éª¤ã€‚æ­¤å¤–ï¼Œè¿™äº›é¢å¤–çš„æ­¥éª¤å¿…é¡»åœ¨CPUä¸Šæ‰§è¡Œï¼Œè¿™åè¿‡æ¥å¯èƒ½éœ€è¦æ…¢é€Ÿçš„è®¾å¤‡é—´æ•°æ®ä¼ è¾“ã€‚å¿«é€Ÿä½¿ç”¨è¾…åŠ©æ¨¡å‹å¯¹äºå®ç°è¾…åŠ©ç”Ÿæˆçš„å¥½å¤„è‡³å…³é‡è¦ã€‚

æœ€åï¼Œå¯å‘å¼æ–¹æ³•ã€‚åˆ°è¿™ä¸€ç‚¹ï¼Œæ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°è¾…åŠ©ç”Ÿæˆä¸ç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹ä¹‹é—´çš„ç›¸ä¼¼ä¹‹å¤„â€”â€”æ¯•ç«Ÿï¼Œæ‚¨æ­£åœ¨æ–‡æœ¬ç”Ÿæˆä¸­è¿è¡Œæ–‡æœ¬ç”Ÿæˆã€‚æ¯ä¸ªå€™é€‰tokenéƒ½ä¼šæœ‰ä¸€ä¸ªè¾…åŠ©æ¨¡å‹æ¨¡å‹çš„å‰å‘ä¼ é€’ï¼Œè€Œæˆ‘ä»¬çŸ¥é“å‰å‘ä¼ é€’æ˜¯æ˜‚è´µçš„ã€‚è™½ç„¶æ‚¨æ— æ³•é¢„å…ˆçŸ¥é“è¾…åŠ©æ¨¡å‹æ¨¡å‹å°†æ­£ç¡®è·å–çš„tokenæ•°é‡ï¼Œä½†æ‚¨å¯ä»¥è·Ÿè¸ªæ­¤ä¿¡æ¯å¹¶ä½¿ç”¨å®ƒæ¥é™åˆ¶è¯·æ±‚ç»™è¾…åŠ©æ¨¡å‹çš„å€™é€‰tokençš„æ•°é‡ - è¾“å‡ºçš„æŸäº›éƒ¨åˆ†æ¯”å…¶ä»–éƒ¨åˆ†æ›´å®¹æ˜“é¢„æµ‹ã€‚

ç®—æ³•çš„å…·ä½“æ­¥éª¤ä¸ºï¼š 
* ä½¿ç”¨è´ªå©ªè§£ç ç”ŸæˆåŠ©æ‰‹æ¨¡å‹çš„ä¸€å®šæ•°é‡çš„å€™é€‰tokenï¼Œäº§ç”Ÿå€™é€‰é¡¹ã€‚ç¬¬ä¸€æ¬¡è°ƒç”¨è¾…åŠ©ç”Ÿæˆæ—¶ï¼Œç”Ÿæˆçš„å€™é€‰tokenæ•°é‡åˆå§‹åŒ–ä¸º5ã€‚
* ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹å¯¹å€™é€‰é¡¹è¿›è¡Œå‰å‘ä¼ é€’ï¼Œè·å–logitsã€‚
* ä½¿ç”¨tokené€‰æ‹©æ–¹æ³•ï¼ˆå¯¹äºè´ªå©ªæœç´¢ä½¿ç”¨.argmax()ï¼Œå¯¹äºé‡‡æ ·ä½¿ç”¨.multinomial()ï¼‰ä»logitsä¸­è·å–next_tokensã€‚
* å°†next_tokensä¸å€™é€‰tokenè¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è·å–åŒ¹é…tokençš„æ•°é‡ã€‚è¯·è®°ä½ï¼Œæ­¤æ¯”è¾ƒå¿…é¡»æŒ‰ç…§ä»å·¦åˆ°å³çš„å› æœå…³ç³»è¿›è¡Œï¼šåœ¨ç¬¬ä¸€æ¬¡ä¸åŒ¹é…åï¼Œæ‰€æœ‰å€™é€‰é¡¹éƒ½æ— æ•ˆã€‚
* ä½¿ç”¨åŒ¹é…tokençš„æ•°é‡æ¥åˆ‡åˆ†æ•°æ®ï¼Œå¹¶ä¸¢å¼ƒä¸æœªç¡®è®¤çš„å€™é€‰tokenç›¸å…³çš„å˜é‡ã€‚å®è´¨ä¸Šï¼Œåœ¨next_tokensä¸­ä¿ç•™åŒ¹é…tokenä»¥åŠç¬¬ä¸€ä¸ªä¸åŒçš„tokenï¼ˆæˆ‘ä»¬çš„æ¨¡å‹ä»æœ‰æ•ˆçš„å€™é€‰å­åºåˆ—ä¸­ç”Ÿæˆï¼‰ã€‚
* è°ƒæ•´ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­è¦ç”Ÿæˆçš„å€™é€‰tokençš„æ•°é‡ã€‚æˆ‘ä»¬çš„åŸå§‹å¯å‘å¼æ–¹æ³•ï¼Œå¦‚æœæ‰€æœ‰tokenéƒ½åŒ¹é…ï¼Œåˆ™å¢åŠ 2ï¼Œå¦åˆ™å‡å°‘1ã€‚

æˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹ä¸€ä¸ªç¤ºä¾‹ï¼Œå¦‚æœæ’­æ”¾æœ‰é—®é¢˜å¯ä»¥å¦å­˜ä¸ºä¸‹è½½è§‚çœ‹ï¼Œ[ä¸‹è½½åœ°å€](/img/textgen/gif_4_1080p.mp4)ã€‚
 
 

åœ¨ä¸Šé¢çš„ä¾‹å­ï¼Œæ¯”å¦‚promptæ˜¯"The quick brown"ï¼Œæˆ‘ä»¬é¦–å…ˆè®©è¾…åŠ©æ¨¡å‹ç”¨è´ªå¿ƒç®—æ³•ç”Ÿæˆ5ä¸ªtokenï¼Œå‡è®¾å®ƒç”Ÿæˆçš„æ˜¯"fox jumps into the"ã€‚ç„¶åæˆ‘ä»¬æŠŠå®ƒä»¬æ‹¼èµ·æ¥å˜æˆ"The quick brown fox jumps into the"ç»™å¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒçš„é¢„æµ‹æ˜¯"fox jumps over a"ã€‚æˆ‘ä»¬å‘ç°ä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹ç›¸åŒçš„å‰ç¼€æ˜¯"fox jumps"ï¼Œåˆ°ç¬¬3ä¸ªtokenå°±ä¸ç›¸åŒäº†ã€‚æˆ‘ä»¬å½“ç„¶æ›´ä¿¡ä»»å¤§æ¨¡å‹çš„ç»“æœï¼Œå› æ­¤é¢„æµ‹æ˜¯"The quick brown fox jumps over"ã€‚æ¥ç€æŠŠè¿™ä¸ªåºåˆ—å†äº¤ç»™è¾…åŠ©æ¨¡å‹ã€‚å¾ªç¯ä¸Šé¢çš„è¿‡ç¨‹ç›´åˆ°ç”Ÿæˆç»“æŸã€‚å½“ç„¶è¿™ä¸ªè¿‡ç¨‹å¯èƒ½ä¼šè°ƒèŠ‚å€™é€‰tokençš„æ•°é‡ï¼Œæ¯”å¦‚ä½¿ç”¨ç®€å•çš„å¯å‘ï¼šå¦‚æœæ‰€æœ‰tokenéƒ½åŒ¹é…ï¼ŒåŠ 2ï¼Œå¦åˆ™å‡å°‘1ï¼Œè¿™é‡Œçš„ä¾‹å­å°±æ˜¯å‡1å˜æˆ4ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸‹ä¸€æ¬¡æŠŠ"The quick brown fox jumps over"ç»™è¾…åŠ©æ¨¡å‹ï¼Œå®ƒåªéœ€è¦è¾“å‡º4ä¸ªtokenã€‚

æˆ‘ä»¬åˆ†æä¸€ä¸‹ï¼Œæœ€å¥½çš„æƒ…å†µæ˜¯ä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹å®Œå…¨ä¸€æ ·ï¼Œè¿™ä¸ªæ—¶å€™åªéœ€è¦å¤§çš„æ¨¡å‹è¿›è¡Œä¸€æ¬¡forwardå°±èƒ½ç”Ÿæˆ5ä¸ªtoken(è€ŒåŸæ¥åªèƒ½ç”Ÿæˆä¸€ä¸ªï¼Œå½“ç„¶å°æ¨¡å‹è¿˜å¾—forward 5æ¬¡ï¼Œä¸è¿‡æˆ‘ä»¬ä¸€èˆ¬é€‰æ‹©çš„å°æ¨¡å‹é€Ÿåº¦æ¯”å¤§æ¨¡å‹å¿«ä¸€ä¸ªæ•°é‡çº§)ã€‚æœ€åçš„æƒ…å†µå‘¢ï¼Ÿä¸¤ä¸ªæ¨¡å‹ä»ç¬¬ä¸€ä¸ªtokenå¼€å§‹é¢„æµ‹ç»“æœå°±ä¸ç›¸åŒï¼Œé‚£ä¹ˆå¤§æ¨¡å‹forwardä¸€æ¬¡è¾“å‡º5ä¸ªtokenï¼Œåªæœ‰ä¸€ä¸ªæ˜¯æœ‰ç”¨çš„(å› ä¸ºç¬¬2åˆ°ç¬¬5ä¸ªtokençš„è¾“å…¥ä¸æ˜¯è‡ªå›å½’äº§ç”Ÿï¼Œè€Œæ˜¯å°æ¨¡å‹ç»™çš„ï¼Œå› ä¸ºç¬¬ä¸€ä¸ªå°±ä¸èƒ½ç”¨ï¼Œæ‰€ä»¥åé¢çš„è®¡ç®—éƒ½ä½œåºŸäº†)ã€‚



## å‚è€ƒèµ„æ–™

* [ Text generation strategies](https://huggingface.co/docs/transformers/generation_strategies)

* [How to generate text: using different decoding methods for language generation with Transformers ](https://huggingface.co/blog/how-to-generate)

* [Guiding Text Generation with Constrained Beam Search in ğŸ¤— Transformers](https://huggingface.co/blog/constrained-beam-search)

* [Assisted Generation: a new direction toward low-latency text generation ](https://huggingface.co/blog/assisted-generation)

* [Utilities for Generation](https://huggingface.co/docs/transformers/internal/generation_utils)
