---
layout:     post
title:      "Continual Pre-training of Language Models论文解读" 
author:     "lili" 
mathjax: true
sticky: false
excerpt_separator: <!--more-->
tags:
    - Pre-training
    - LLM
    - CPT
---

本文是论文[Continual Pre-training of Language Models](https://arxiv.org/abs/2302.03241)的解读。我们如果需要对Llama 2等预训练模型进行继续领域预训练，通常会面临灾难性遗忘(catastrophic forgetting, CF)。最常见的方法是在领域数据中混入一些原始的训练数据，但是这要求有原始的预训练数据，而且也会增加训练成本。而Llama 2并没有开源其预训练数据。本文的方法比较有趣，它不要求原始的预训练数据。

<!--more-->

**目录**
* TOC
{:toc}


## Abstract
 
本文研究语言模型的继续预训练(continual pre-training)，更具体的说是继续领域自适应预训练(continual domain-adaptive pre-training, continual DAP-training)。之前的研究证明用领域的数据对预训练模型继续预训练之后能提升对应领域的最终性能。本文提出了一种新的方法来做继续领域自适应预训练，它的输入是一系列不同领域的未标注数据，而且也不知道具体有哪些领域。本文的创新点是提出了soft-masking机制来直接控制LM的更新。一种新的proxy被提出来用于保留原始LM的通用能力。此外，通过之前LM和新的LM的对照(contrast)学习来实现知识的整合。这不但可以避免灾难性遗忘，而且可以实现原始LM到新LM的知识转移(knowledge transfer)从而提升最终任务的性能。

## Introduction

预训练语言模型如BERT和RoBERTa已经显著推动了自然语言处理（NLP）的发展。最近，许多持续学习( continual learning ,CL)系统也利用这些语言模型来逐步学习一系列终端任务(end task)，我们称之为持续终端任务学习。持续地预训练语言模型本身也是可取的。这包括（1）持续的通用预训练，使用与预训练数据具有相似分布的最新数据，逐步更新语言模型；（2）持续的领域自适应预训练，进一步逐步预训练语言模型以使其适应一系列领域。请注意，语言模型编辑（language model edit）用于纠正语言模型中学到的错误，是持续终端任务学习的一个特例，因为每个编辑任务或一组编辑任务一起学习基本上是持续学习中的一个任务，其目标是在不干扰或遗忘已学到的其他知识的情况下正确执行编辑。

本文关注持续领域自适应预训练（DAP-training）。在终端任务微调之前，使用大规模未标记的领域语料库进行DAP-training能够取得更好的结果。本文更进一步，持续学习以提高语言模型处理新领域或主题的能力，而不会遗忘过去学到的技能或知识。在现实世界中，数据不断变化，新的领域、事件或主题不断涌现，LM需要更新以更好地为用户提供服务，这一点至关重要。
 
 
我们将这个问题称为持续领域自适应预训练（continual DAP-training）。从一个预训练的通用LM开始（LM已经在$D_0$上进行了预训练），我们逐步对一系列领域语料库$D_1, D_2$等进行领域自适应预训练。一旦一个领域被训练，其数据就不再可访问。这与传统的持续学习（CL）不同，其中每个任务都是一个终端任务。在提出的持续领域自适应预训练中，每个任务是要学习的未标记领域语料库。最后才有终端任务微调来评估其性能。值得注意的是，$D_0$通常是一个广泛或通用的领域。在实践中，一个持续领域自适应预训练的LM可以由个别用户、机构或二者混合训练，他们拥有一种或多种特定领域的大型语料库。在这种情况下，原始数据可能不会被共享，但最终的模型可以被所有人共享。

我们期望一个持续领域自适应预训练系统有如下特性：(1) 它不应遭受灾难性遗忘（CF），即在已学领域上应表现得相当好。这要求系统要避免通用语言知识的CF。这很重要，因为仅从每个领域学到的知识将不足以实现良好的终端任务。(2) 它应鼓励知识跨领域传递（konwledge transfer, KT），以实现改善的端任务性能。这要求系统能够进行前向传递，通过利用先前领域的知识来学习新领域，以及反向传递，在学习相关的新领域后在先前领域上取得改进的性能。(3) 它应在不需要每个端任务微调的领域标识，也就是测试的适合不告诉它任务来自哪个领域。


现有的持续学习方法中没有一种能够实现上述所有目标。本文是朝着实现这些目标迈出的一步。本文提出了DAS算法。DAS提出了一种新颖的soft-masking机制，它会计算某个单位(注意力头或者全连接网络的单个神经元)对于通用或领域知识的重要性（一个介于0和1之间的实数），并根据其重要性值对它们进行soft-masking，以控制反向梯度流。在前向传播中，不应用soft-masking，这鼓励跨领域的知识传递。它不会因为某个领域而隔离任何子网络，以便可以利用完整LM中的知识进行终端任务微调。

为了应用这个机制，DAS实现了两个功能：(1) 初始化，计算LM中与通用知识相关的单位的重要性，而无需访问LM的预训练数据($D_0$)。在持续学习开始之前，它应用于预训练的LM。 (2) 持续学习，DAS对每个领域进行DAP训练，同时防止通用和领域知识的灾难性遗忘，并鼓励跨领域的知识传递。在（1）中，如何在没有预训练数据的情况下计算重要性并不明显。DAS提出了一种基于鲁棒性的新型代理(proxy)，用于计算与通用知识相关的单位的重要性。在（2）中，soft-masking是直接适用的，因为我们有领域数据，并且可以根据其梯度计算重要性，这受到了修剪(pruning)领域的启发。此外，DAS对先前学到的知识和完整的知识(完整知识包括先前学到的领域和当前领域)进行对比，以鼓励当前领域表示学习先前领域中尚未具备的知识，并将其与已学知识整合。在终端任务微调中，DAS不需要知道任务是哪个领域，因为所有知识都积累到了DAP训练的LM中。

##  Related Work

### DAP-training

DAP训练可以通过直接更新LM或仅训练一小组额外参数来实现。比常见的是训练训练了一个adapter或者prompt以适应新领域。虽然它们可能是有效的，但在这些额外模块之间传递知识通常具有挑战性，并且可能不准确。DAS属于直接更新LM的前者，由于灾难性遗忘（CF），这对于持续学习而言非常具有挑战性。

### Continual learning

大多数持续学习方法都是为了克服灾难性遗忘，其中包括：（1）正则化方法。计算每个参数对先前任务的重要性，并使用正则化器对变化的总和进行惩罚。DAS与[EWC](https://arxiv.org/abs/2105.04093)相关但也有很大的区别。 DAS不控制每个参数/权重，而是基于它们的重要性分数控制注意力头或神经元。另外，DAS直接控制每个神经元的反向梯度流，这比所有参数的变化总和更细粒度和有效。我们的实验结果证实EWC明显劣于DAS
（2）重播(Replay)方法。这些方法保留或生成一些旧任务的数据并在学习新任务时使用它们；（3）参数隔离方法，为不同任务/领域分配神经元和参数或子网络，并在任务学习中对它们进行屏蔽。对于持续领域自适应预训练，这意味着终端任务无法使用LM中的通用知识，从而导致端任务性能较差。

在自然语言处理领域，持续学习已经应用于槽填充(slot filling)、语言学习、情感分析、主题建模、问答以及文本分类等任务。但是，目前还没有应用于DAP-training的相关工作。一些最近的持续学习论文涉及到语言模型。在[Continual Learning in Task-Oriented Dialogue Systems](https://arxiv.org/abs/2012.15504)中，系统为不同领域学习单独的适配器，因此没有灾难性遗忘或知识传递问题。[DEMIX](https://arxiv.org/abs/2108.05036)用最接近的旧适配器初始化新适配器。[CPT](https://arxiv.org/abs/2210.05549)和[ELLE](https://arxiv.org/abs/2203.06311)与DAS最相关。然而，CPT使用参数隔离方法来学习和保护每个任务，这是相对较弱的。它还需要在终端任务微调中使用领域ID。ELLE必须从预训练LM本身开始，而不是从像DAS这样的预训练LM开始。它还使用大量内存（每个领域1G）来存储replay数据（包括预训练数据）并为每个领域扩展网络，而在DAS中这两者都是不需要的。Jin等在[Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora](https://arxiv.org/abs/2110.08534)评估了几种与DAS类似的环境中现有的持续学习技术，并进行了有关处理灾难性遗忘的分析。然而，在该论文中并没有提出新的技术。

### Neural network pruning

网络中的许多参数是冗余的，可以进行剪枝。现有的方法包括丢弃具有较小绝对值的参数、累积梯度和[lottery ticket假设](https://arxiv.org/abs/2005.03454)。然而，这些方法并不直接适用，因为我们需要保留的不仅仅是单个领域的知识，还有LM中的通用知识。对于通用知识，由于我们没有任何预训练数据，因此提出了一种基于鲁棒性的代理。对于领域知识，我们采用了一种剪枝方法，但使用重要性作为soft-masking，因为我们希望累积知识而不是压缩LM。


### Contrastive Learning.

对比学习通过最大化正样本的相似性并最小化负样本的相似性来学习好的表示。

$$
L_{\text{contrast}} = - \frac{1}{N}\sum_{n=1}^N log \frac{e^{sim(q_n, q_n^+)/\tau}}{\sum_{j=1}^Ne^{sim(q_n, q_j^+)/\tau}}
$$

其中，N是批处理大小，$\tau$是温度参数，sim(·)是相似性度量，$q_n$和$q_n^+$是正样本对($x_n, x_n^+)$的表示。DAS通过对比先前领域和预训练LM（通用知识）学到的知识与完整知识（包括先前领域和当前领域知识）来实现一种互补效果。

也就是说，对于一个batch的数据，我们可以找到正样本和负样本(后面会讲怎么找)，然后这个loss的目标是使分子尽量大，也就是正样本对的向量比较接近，而负样本对距离较远。

## DAS

DAS中的持续领域自适应预训练基于两个主要思想：(1) 通过基于重要性对单元进行soft-masking，以保留LM中的重要通用语言知识以及从先前领域学到的知识，以克服灾难性遗忘，同时促进跨任务的知识传递; (2) 鼓励模型学习当前领域和先前领域的互补表示，以实现知识整合。下图概述了DAS的整体思路。这里大家简单看一下就行，下面会详细介绍。


<a id="img1">![](/img/das/1.png)</a> 


整个学习过程包括两个主要部分：初始化和持续学习。 初始化计算每个单元对LM中的通用语言知识的重要性，它在持续学习开始之前完成。 持续学习包括两个步骤：领域训练和重要性计算。 领域训练：利用到目前为止积累的重要性分数（包括对原始LM中通用知识和先前领域学到的知识的重要性）以及当前领域的输入数据来学习该领域。重要性计算：计算当前领域的重要性分数，以备将来使用。下面详细介绍每个步骤。

### 初始化

初始化函数计算Transformer中的单元（注意力头和神经元）对原始LM中的通用知识的重要性。Transformer的关键组件包括多头注意力层、中间层和输出层。在下文中，我们使用“层”(l)来表示这三个层中的任何一个，因为我们的方法对这三个层进行了类似的处理。

#### 一层中的重要单元

一层中的所有单元(全连接层的一个输出神经元或者多头注意力机制的一个头)并不是同等重要的。我们引入一个虚拟参数(virtual parameter) $g_l$，来衡量一层中不同单元的重要性。一开始我们把它初始化为1，因此大家都是同等重要。我们把$g_l$乘以l层原来的输出$o_l$得到新的输出：

$$
\hat{o}_l = g_l \otimes o_l
$$

其中$o_l$是l层的输出，对于全连接层来说就是每一个神经元的输出。$\otimes$是element-wise的乘法。如果$g_l=[1,....,1]$，那么这个网络的结果和原来没有任何区别。给定数据集$D=\{ (x_n,y_n) \}_{n=1}^N$，我们用下式计算每个神经元的重要性：

$$
I_l = \frac{1}{N} \sum_{n=1}^N | \frac{\partial \mathcal{L}_{\text{impt}}(x_n,y_n)}{\partial g_l} |
$$

其中$\mathcal{L}_{\text{impt}}$是任务相关的损失函数。上式的意思就是用每一个训练数据的损失函数对$g_l$求梯度然后求和，这就可以看成某个神经元或者注意力头的重要性。注意：$g_l$的值是永远为1的，在后面的学习过程中不会用梯度下降更新它。【注：根据链式法则，$\frac{\partial \mathcal{L}}{\partial g_l} = \frac{\partial \mathcal{L}}{\partial \hat{o_l}} \frac{\partial \hat{o_l}}{\partial g_l}=\frac{\partial \mathcal{L}}{\partial \hat{o_l}} o_l$。如果不引入$g_l$，那么直接求$\frac{\partial \mathcal{L}}{\partial  o_l}$，然后乘以$o_l$就可以得到其重要性。】$I_l$的维度和$g_l$是一样的，每一个值对应某个神经元或者注意力头。

初始化函数的目的是学习不同单位对通用知识的语言模型（表示为$I_l^{(0)}$)中的重要性。虽然上式提供了一种可能的方法，但它并不直接适用。如果我们使用手头的领域数据并采用MLM损失作为$\mathcal{L}_{\text{impt}}$，则$\nabla g_l$只会计算这个单元对于领域特定知识的重要性。然而，为了计算单位对语言模型中通用知识的重要性，我们需要用于预训练语言模型的原始数据来计算。在实践中，用户无法访问这样的数据。此外还需要标签，但我们在DAP训练中使用的领域语料库是无标签的。

为了解决这些问题，我们提出了一个代理KL散度损失( $\mathcal{L}_{\text{proxy}}$ )来替代$$\mathcal{L}_{\text{impt}}$$，以学习不同单位对通用知识的重要性。

#### 代理KL散度损失

我们可以使用模型的稳健性作为代理，即我们尝试检测对语言模型稳健性重要的单元。它们的梯度，$\nabla g_l$，说明了稳健性和对语言模型的重要性。我们的理由是：如果一个$$I_{l,j}^{(0)}$$（在l层中单元i的重要性）具有较高的值，那么它对语言模型的稳健性很重要，因为它的变化可能导致语言模型发生较大变化。因此，它是一个重要的单元。相反，如果$$I_{l,j}^{(0)}$$较小，它就是一个不太重要的单元。为了计算语言模型的稳健性，我们从当前领域数据的子集$x_n^{\text{sub}}$中（在DAP训练中没有标签）取样，并将$x_n^{\text{sub}}$输入语言模型两次，以获取它的两个表示，然后计算它们之间的KL散度：

$$
\mathcal{L}_{\text{impt}} = \text{KL}(f_{\text{LM}}^1(x_n^{\text{sub}},f_{\text{LM}}^2(x_n^{\text{sub}})
$$

其中$f_{\text{LM}}^1$和$f_{\text{LM}}^2$是同一个LM使用不同的dropout的结果。我们不需要做额外的工作，因为Transformer的全连接层和注意力层都有dropout。因此，只需要把同一个输入计算两次就可以得到不同的输出。因此这两者的不同可以被看成是LM的稳健性。所以如果某个单元对于模型的稳健性作用大，那么就说明这个单元是更重要的。【关于$x_n^{\text{sub}}$，我个人认为最好还是选择跟原始LM的分布比较接近的比较好。论文中使用领域的数据的一个子集，可能还是跟原始LM的分布有一些区别。】

### 训练：通过soft-masking和对比损失进行新领域继续预训练

回顾一下，在进行领域自适应训练时，我们希望使用在学习第t个领域时累积的重要性$$I_l^{(\le t-1)}$$来保留语言模型中学到的知识，其中包括通用知识$I_l^{(0)}$和每个已学过的领域k的学到的领域特定知识$I_l^{(k)}$（k属于{1...t-1}）。这通过根据累积的重要性进行soft-masking学习来实现。【原文注：在训练前，我们会把同一层的不同单元的重要性归一化到成均值0方差1的分布，然后使用tanh函数把它转换到[0,1]之间的值，这样就可以作为soft的mask】

#### 累计重要性

在学习完领域t-1之后，我们通过逐元素最大值(EMax)进行重要性的累积，具体操作如下：

$$
I_l^{(\le t-1)} = \text{EMax}(I_l^{(t-1)}, I_l^{(\le t-2)})
$$

我们不需要保存$I^0_l$和$$\{I_l^{(k)} \}_{k=1}^{t-1}$$，我们只需要增量的保存累计重要性。

#### 对不同单元进行soft-masking

给定层l的累积重要性$I_l^{(\le t-1)}$和DAP训练损失$$\mathcal{L}_{\text{DAP-train}}$$（通常是MLM损失），我们对其相应的梯度（$\nabla_l$）进行soft-masking，具体如下：

$$
\hat{\nabla_l} = (1-I_l^{(\le t-1)}) \otimes \nabla_l
$$

上式的意思很简单：如果某个单元比较重要，那么就尽量不用更新它，这是通过$(1-I_l^{(\le t-1)})$来实现的，如果某个单元非常重要(重要性为1)，那么$$\hat{\nabla_l}=0$$，也就是完全不更新参数。反正如果它的重要性是0，那么就正常用梯度更新。实际的重要性通常是[0,1]之间的值，所以这是为什么叫做soft-masking的原因。

上面的soft-masking仅在反向传播中应用，而在正向传播中则不应用，这有助于知识传递，因为每个域的训练都可以利用从所有过去域学到的知识。为了进一步鼓励模型从累积知识（$$I_l^{(\le t-1)}$$）和完整知识（累积和当前域知识）中学到一个良好的表示，我们引入了对比学习方法以促进互补表示。

#### 整合先前学到的知识与当前域知识

soft-masking有助于防止遗忘先前学到的知识。我们希望通过整合新学到的知识进一步促进知识传递。我们提出对先前学到的知识和完整知识（先前学到的知识和当前域知识）进行对比。请注意，对比不能对共享的过去知识产生影响，因为它受到soft-maksing的保护。因此，它有效地将当前域知识推开，使其与过去知识互补。这是基于当前域数据完成的。

#### 对学到的知识和完整知识进行对比

我们将没有考虑重要性的LM输出表示为$$o^{\text{full}}$$，指的是完整知识。我们进一步将经重要性相乘的LM输出（$$I_l^{(\le t-1)} \otimes o_l$$）表示为$$o^{\text{prev}}$$，指的是先前学到的知识。我们通过以$$o^{\text{full}}$$作为锚点和使用不同的dropout作为正样本（表示为$$o^{\text{full+}}$$）的$$o^{\text{full}}$$进行对比。$$o^{\text{prev}}$$被用作负实例。

$$
\mathcal{L}_{\text{contrast}} = - \frac{1}{N} \sum_{n=1}^N log \frac{e_n^{sim(o^{\text{full}}, o_n^{\text{full+}})/\tau}}{\sum_{j=1}^N(e^{sim(o_n^{\text{full}}, o_j^{\text{full+}})/\tau} + e^{sim(o_n^{\text{full}}, o_j^{\text{prev+}})/\tau})}
$$

【上式看起来有点复杂，其实思路很简单，就是让模型学习的方向是使得完整知识和之前知识差距尽量大，也就是要学新知识。】

<a href="#img1">上图</a>（B）显示了一条红色箭头从ofull指向自身，表示正实例来自两次输入。指向$$o^{\text{prev}}$$的虚线红色箭头表示对比完整和先前学到的知识的负实例。

#### 最终的损失函数
最终的DAP训练损失结合了在应用了用于通用知识的提出的soft-masking语言模型（MLM）损失对比损失（λ是一个超参数）:


$$
\mathcal{L}_{\text{DAP-train}} = \mathcal{L}_{\text{MLM}} + \lambda \mathcal{L}_{\text{contrast}}
$$

### 计算当前领域单元的重要性

在对新的/当前域t进行训练之后，我们通过应用等式3来学习该域的单元重要性。与等式4中需要代理计算$$\mathcal{L}_{\text{impt}}$$不同，因为我们可以直接使用当前领域数据，无需任何代理。具体而言，我们随机采样当前域数据的子集（一个超参数）{($$x^{\text{sub}}_n, y^{\text{sub}}_n$$)}, 其中$$x^{\text{sub}}_n$$是输入，$$y^{\text{sub}}_n$$是mask后的token，就像MLM自监督损失中一样。然后，通过将LMLM代入等式3中的$$\mathcal{L}_{\text{MLM}}$$，我们可以轻松地计算重要性$$I_l^{(t)}$$。得到的$$I_l^{(t)}$$将在下一个任务中累积到先前累积的重要性并进行学习的soft-masking。
