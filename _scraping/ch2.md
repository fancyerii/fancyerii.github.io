---
layout:     post
title:      "第二章：The Legalities and Ethics of Web Scraping"
author:     "lili"
mathjax: true
sticky: false
excerpt_separator: <!--more-->
tags:
    - python
    - scraping
---

<!--more-->
**目录**
* TOC
{:toc}

2010年，软件工程师Pete Warden构建了一个网络爬虫来收集来自Facebook的数据。他从大约2亿Facebook用户那里收集了数据——姓名、位置信息、朋友和兴趣。当然，Facebook注意到了并向他发送了停止侵权的信函，他遵守了。当被问及为何遵守时，他说：“大数据？便宜。律师？不便宜。”

在本章中，您将了解与网络爬虫相关的美国法律（以及一些国际法律），并学习如何分析特定网络爬虫情境的合法性和伦理问题。

在阅读以下内容之前，请考虑一个显而易见的事实：我是软件工程师，不是律师。不要将您在这里或书中任何其他章节中阅读的内容解释为专业法律建议，也不要因此而采取行动。尽管我相信自己能够知识渊博地讨论网络爬虫的合法性和伦理，但在进行任何法律上有歧义的网络爬虫项目之前，您应咨询律师（而不是软件工程师）。

本章的目标是为您提供一个框架，以便您能够理解和讨论网络爬虫合法性的各个方面，如知识产权、未经授权的计算机访问和服务器使用，但这不应替代实际的法律建议。

## 商标、版权、专利，天哪！

现在是知识产权速成课程时间！知识产权有三种基本类型：商标（有时用TM或®符号表示）、版权（常用©表示）和专利（常用专利号或不使用任何标志）。

专利仅用于声明对发明的所有权。您不能为图像、文本或任何信息本身申请专利。尽管某些专利（如软件专利）比我们所认为的“发明”更不具体，但请记住，专利的是事物（或技术），而不是构成软件的数据。除非您是从抓取的图表中构建东西，或者有人为网络爬虫的方法申请了专利，否则您不太可能因网络爬虫而无意中侵犯专利。

商标也不太可能成为问题，但仍需考虑。根据美国专利商标局的说法：

商标是用来识别和区分一个方的商品来源与他方商品来源的文字、短语、符号和/或设计。服务标记是用来识别和区分服务来源而非商品的文字、短语、符号和/或设计。术语“商标”通常用于指代商标和服务标记。

除了您想到的文字和符号外，其他描述性属性也可以注册为商标。例如，容器的形状（如可口可乐瓶）或甚至颜色（最著名的是Owens Corning的粉红豹玻璃纤维绝缘材料的粉红色）。

与专利不同，商标的所有权在很大程度上取决于使用的上下文。例如，如果我希望发布一篇附带可口可乐标志图片的博客文章，我可以这么做，只要我不暗示我的博客文章是由可口可乐赞助或发布的。如果我想制造一种新软饮料，并在包装上展示相同的可口可乐标志，那显然是商标侵权。同样，虽然我可以将我的新软饮料包装成粉红豹粉红色，但我不能用这种颜色制造家庭绝缘产品。

这使我们进入“合理使用”话题，这通常在版权法的背景下讨论，但也适用于商标。作为对品牌的参考储存或显示商标是可以的。以可能误导消费者的方式使用商标是不可以的。然而，“合理使用”的概念不适用于专利。例如，一个行业中的专利发明不能在另一个行业中应用，除非与专利持有者达成协议。

### 版权法

商标和专利有一个共同点，即它们必须正式注册才能被认可。与普遍的误解相反，这对于版权材料并不适用。什么使图像、文本、音乐等受版权保护？不是页面底部的“版权所有”警告或“已发布”与“未发布”材料之间的任何特殊之处。您创建的每一件材料在您将其带入存在的那一刻就自动受到版权法的保护。

《保护文学和艺术作品伯尔尼公约》以1886年首次通过的瑞士伯尔尼命名，是版权的国际标准。该公约基本上规定，所有成员国必须像对待本国公民的作品一样承认其他成员国公民作品的版权保护。实际上，这意味着，作为美国公民，您可以在美国因侵犯例如法国某人所写材料的版权而被追究责任（反之亦然）。

**版权注册**

虽然确实版权保护自动适用，不需要任何形式的注册，但也可以在美国政府正式注册版权。这通常针对有价值的创意作品（如大型电影）进行，以便以后使任何诉讼更容易，并创建强有力的文件记录以证明谁拥有作品。然而，不要让这种版权注册的存在让您感到困惑——所有创意作品，除非特别属于公共领域，都是受版权保护的！

显然，版权比商标或专利更令网络爬虫担忧。如果我从别人的博客抓取内容并发布到自己的博客，我很可能会面临诉讼。幸运的是，我有几个保护层，可能使我的博客爬虫项目具有辩护性，具体取决于其功能。

首先，版权保护仅适用于创意作品，不包括统计数据或事实。幸运的是，许多网络爬虫所追求的是统计数据和事实。收集各网站诗歌并在您自己的网站上展示这些诗歌的网络爬虫可能会违反版权法；然而，收集各网站上诗歌发布频率信息的网络爬虫则不会。诗歌本身是一种创意作品。按月发布在网站上的诗歌平均字数是事实数据，而不是创意作品。

逐字发布内容（而不是从原始抓取数据中汇总/计算的内容）可能不违反版权法，如果该数据是价格、公司高管姓名或其他事实性信息。

即使是受版权保护的内容也可以在一定程度上直接使用，1988年《数字千年版权法》对此有所规定。《数字千年版权法》很长，包含许多具体规则，涵盖从电子书到电话的各个方面。然而，其中两个主要观点可能与网络爬虫特别相关：

* 根据“避风港”保护，如果您从一个您被引导相信只包含无版权材料的来源抓取材料，但用户提交了版权材料，只要您在被通知后移除该版权材料，您就受到保护。
* 您不能绕过安全措施（如密码保护）来收集内容。

此外，《数字千年版权法》还承认《美国法典》第17条第107节中的合理使用原则，如果使用版权材料符合合理使用原则，则不应根据避风港保护发出删除通知。

简而言之，您不应在未经原作者或版权持有人许可的情况下直接发布版权材料。如果您将有自由访问权限的版权材料存储在自己的非公开数据库中进行分析，这是可以的。如果您将该数据库发布到您的网站供查看或下载，这是不可以的。如果您分析该数据库并发布关于字数、按多产度排列的作者列表或其他数据的元分析，这是可以的。如果您在元分析中附上一些精选的引文或简短的数据样本以说明您的观点，这也可能是可以的，但您可能需要查阅《美国法典》中的合理使用条款以确保无误。

**版权与人工智能**

生成式人工智能，即基于现有创作作品集生成新“创作”作品的 AI 程序，在版权法中提出了独特的挑战。

如果生成式 AI 程序的输出与现有作品相似，则可能存在版权问题。许多案例已经作为先例来指导“相似”一词的含义，但根据国会研究服务处的说法：

实质性相似性测试很难定义，并且在美国各地法院的定义各不相同。法院各种描述了该测试的要求，例如，作品必须具有“实质上相似的整体概念和感觉”或“整体外观和感觉”，或“普通的合理人会无法区分两者”。

现代复杂算法的问题在于，很难自动确定您的 AI 是否产生了一个令人兴奋且新颖的混搭作品，还是更为……直接的衍生作品。AI 可能无法将其输出标记为与特定输入“实质上相似”，甚至无法识别它用来生成其创作的具体输入！可能第一个迹象是收到停止信函或法庭传票。

除了生成式 AI 输出的版权侵权问题，未来的法院案件还在测试训练过程本身是否可能侵犯版权持有人的权利。

为了训练这些系统，几乎总是需要下载、存储和复制受版权保护的作品。虽然下载受版权保护的图像或文本看起来没什么大不了的，但这与下载受版权保护的电影没有太大区别——而你不会下载电影，对吧？

有些人声称这是合理使用，他们没有以影响市场的方式发布或使用内容。

截至撰写本文时，OpenAI 正在美国专利商标局辩称，其使用大量受版权保护的材料构成合理使用。虽然这个论点主要是在 AI 生成算法的背景下提出的，但我怀疑其结果将适用于各种用途的网页抓取工具。

## 对动产的侵权

对动产的侵权与我们通常所理解的“擅自进入”法律有根本区别，它适用于动产，即法律术语中的动产，而非不动产或土地。当您的财产因某种方式受到干扰而无法访问或使用时，该法律适用。

在云计算时代，我们很容易不将网络服务器视为真实的、有形的资源。然而，服务器不仅由昂贵的组件构成，还需要存储、监控、冷却、清洁并提供大量电力。根据一些估计，全球约有10%的电力消耗来自计算机。如果查看您自己的电子设备不足以让您信服，想想谷歌庞大的服务器农场，所有这些服务器都需要连接到大型电站。

虽然服务器是昂贵的资源，但从法律角度来看，它们很有趣，因为网络管理员通常希望人们使用他们的资源（即访问他们的网站）；他们只是不希望人们过度使用他们的资源。通过浏览器访问网站是可以的，但显然不能对其发起大规模的分布式拒绝服务（DDOS）攻击。

要使网络爬虫违反对动产的侵权，需要满足三个标准：

* 缺乏同意
因为网络服务器对所有人开放，它们通常也“同意”网络爬虫的访问。然而，许多网站的服务条款协议明确禁止使用爬虫。此外，任何发给您的停止信函都可能撤销这种同意。

* 实际损害
服务器是昂贵的资源。除了服务器成本外，如果您的爬虫导致网站崩溃或限制其为其他用户服务的能力，这会增加您造成的“损害”。

* 故意性
如果您在编写代码，您知道它的功能是什么！在为您的网络爬虫辩护时，声称缺乏故意性可能效果不佳。

要使对动产的侵权适用，必须满足所有这三个标准。然而，如果您违反了服务条款协议，但未造成实际损害，不要以为您可以免于法律诉讼。您可能非常有可能违反了版权法、《数字千年版权法案》（DMCA）、《计算机欺诈和滥用法案》（本章后面会详细讨论）或适用于网络爬虫的其他多种法律之一。

**限制您的爬虫**

在过去，网络服务器比个人电脑强大得多。事实上，服务器的定义之一就是“大型计算机”。现在，情况有所变化。例如，我的个人电脑有一个 3.5 GHz 的处理器和 32 GB 的 RAM。而 AWS 的中型实例相对只有 4 GB 的 RAM 和大约 3 GHz 的处理能力。

有了良好的互联网连接和专用机器，即使是一台个人电脑也能对许多网站造成巨大负荷，甚至使其瘫痪或完全崩溃。除非有医疗紧急情况且唯一的解决办法是两秒内汇总 Joe Schmo 网站上的所有数据，否则没有理由对某个网站进行猛烈攻击。

“看着的爬虫永远不会完成。” 有时让爬虫在夜间运行比在下午或晚上运行要好，原因如下：

* 如果您有大约 8 小时时间，即使每页速度缓慢到 2 秒，您也可以爬取超过 14,000 页。当时间不是问题时，您不会被诱惑去加快爬虫的速度。
* 假设网站的目标受众在您的一般位置（对于远程目标受众相应调整），网站的流量负载在夜间可能会大大降低，这意味着您的爬取不会增加高峰时段的拥堵。
* 您可以通过睡觉节省时间，而不是不断检查日志以获取新信息。想想早上醒来看到全新数据的兴奋吧！

请考虑以下场景：

* 您有一个网络爬虫，它遍历 Joe Schmo 的网站，汇总其部分或全部数据。
* 您有一个网络爬虫，它遍历数百个小型网站，汇总其部分或全部数据。
* 您有一个网络爬虫，它遍历一个非常大型的网站，例如维基百科。

在第一个场景中，最好让爬虫在夜间慢速运行。

在第二个场景中，最好以循环方式爬取每个网站，而不是一个接一个慢慢爬取。根据您爬取的网站数量，这意味着您可以以您的互联网连接和机器所能管理的最快速度收集数据，但对于每个远程服务器来说，负载是合理的。您可以通过编程实现这一点，要么使用多线程（每个线程爬取一个网站并暂停其自身的执行），要么使用 Python 列表来跟踪网站。

在第三个场景中，您的互联网连接和家庭机器对像维基百科这样的网站造成的负载可能不会被注意到或在意。然而，如果您使用分布式网络的机器，这显然是另一回事。请小心，并在可能的情况下询问公司代表。



## 计算机欺诈和滥用法案

20世纪80年代初，计算机开始从学术界进入商业领域。病毒和蠕虫首次被视为不只是一个不便（或甚至是一个有趣的爱好），而是一个可能造成经济损失的严重刑事问题。1983年，由马修·布罗德里克主演的电影《战争游戏》也将这个问题带到了公众和时任总统罗纳德·里根的视野中。作为回应，1986年《计算机欺诈和滥用法案》（CFAA）应运而生。

虽然您可能认为CFAA仅适用于那些释放病毒的恶意黑客的刻板印象，但该法案对网络爬虫也有很强的影响。想象一下，一个爬虫扫描网络寻找易猜测密码的登录表单，或者收集意外留在隐藏但公开位置的政府机密。根据CFAA，所有这些活动都是非法的（且理应如此）。

该法案定义了七项主要刑事犯罪，可以概括如下：

* 明知未经授权访问美国政府拥有的计算机并从中获取信息。
* 明知未经授权访问计算机，获取财务信息。
* 明知未经授权访问美国政府拥有的计算机，影响政府对该计算机的使用。
* 明知访问任何受保护的计算机，意图欺诈。
* 明知未经授权访问计算机并对该计算机造成损害。
* 共享或贩卖用于访问美国政府使用的计算机或影响州际或外国商业的计算机的密码或授权信息。
* 试图通过对任何受保护的计算机造成损害或威胁造成损害来敲诈金钱或“任何有价值的东西”。

简而言之：远离受保护的计算机，不要访问您未被授权访问的计算机（包括网络服务器），尤其要远离政府或金融计算机。



## obots.txt 和服务条款

从法律角度来看，网站的服务条款和robots.txt文件处于一个有趣的领域。如果一个网站是公开访问的，网站管理员宣称哪些软件可以访问它，哪些软件不可以，这是有争议的。说“你可以使用浏览器查看这个网站，但不能使用你自己编写的程序查看”是有难度的。

大多数网站在每个页面的页脚都有服务条款（TOS）的链接。TOS不仅包含了针对网络爬虫和自动化访问的规则，还通常包括网站收集什么信息、如何处理这些信息的信息，以及网站提供的服务没有任何明示或暗示的担保的法律免责声明。

如果你对搜索引擎优化（SEO）或搜索引擎技术感兴趣，你可能听说过robots.txt文件。如果你访问任何一个大型网站并查找其robots.txt文件，你会在根目录下找到它：http://website.com/robots.txt。

robots.txt文件的语法是在1994年搜索引擎技术初期发展起来的。大约在那时，像AltaVista和DogPile这样扫描整个互联网的搜索引擎开始与像Yahoo!这样的按主题组织的网站列表竞争。互联网搜索的增长不仅意味着网络爬虫数量的激增，还意味着普通公民可以获取爬虫收集的信息。

虽然今天我们可能认为这种可用性是理所当然的，但一些网站管理员在他们发布在网站深层结构中的信息出现在主要搜索引擎的首页搜索结果时感到震惊。作为回应，robots.txt文件的语法，即机器人排除协议（Robots Exclusion Protocol）被开发出来。

与服务条款不同，服务条款通常用广泛的、非常人性化的语言谈论网络爬虫，而robots.txt可以被自动程序非常容易地解析和使用。虽然这看起来像是一次性解决不受欢迎的机器人问题的完美系统，但请记住：

* robots.txt的语法没有官方的管理机构。它是一个常用且普遍遵循的约定，但没有任何东西可以阻止任何人创建自己的版本的robots.txt文件（除了没有机器人会识别或遵守它，直到它流行起来）。也就是说，它是一个被广泛接受的约定，主要是因为它相对简单，公司没有动力去发明自己的标准或改进它。

* 没有法律或技术手段可以强制执行robots.txt文件。它只是一个标志，表示“请不要访问网站的这些部分”。许多网络爬虫库都遵守robots.txt——尽管这通常是一个可以被覆盖的默认设置。抛开库的默认设置不谈，编写一个遵守robots.txt的网络爬虫实际上在技术上比编写一个完全忽略它的爬虫更具挑战性。毕竟，你需要读取、解析并将robots.txt的内容应用到你的代码逻辑中。

机器人排除协议的语法相当简单。与Python（和许多其他语言）一样，注释以#符号开头，以换行符结束，可以在文件的任何地方使用。

文件的第一行，除了任何注释，都是以User-agent:开头的，指定以下规则适用于哪个用户。接下来是一组规则，根据机器人是否被允许访问网站的那一部分，可以是Allow:或Disallow:。星号（*）表示通配符，可用于描述User-agent或URL。

如果一条规则与其后面的规则相矛盾，最后的规则优先。例如：

```
#Welcome to my robots.txt file!
User-agent: *
Disallow: *

User-agent: Googlebot
Allow: *
Disallow: /private
```

在这种情况下，所有机器人都不允许访问该网站的任何部分，除了Googlebot，它被允许访问除/private目录之外的所有部分。

Twitter（现在也称为“X”）的robots.txt文件对Google、Yahoo!、Yandex（一个流行的俄罗斯搜索引擎）、Microsoft和其他不在任何先前类别中的机器人或搜索引擎有明确的指示。Google部分（看起来与所有其他类别的机器人允许的权限相同）如下所示：

```
#Google Search Engine Robot
User-agent: Googlebot
Allow: /?_escaped_fragment_
Allow: /?lang=
Allow: /hashtag/*?src=
Allow: /search?q=%23
Disallow: /search/realtime
Disallow: /search/users
Disallow: /search/*/grid
Disallow: /*?
Disallow: /*/followers
Disallow: /*/following
```

请注意，Twitter限制访问那些有API的站点部分。因为Twitter有一个管理良好的API（并且可以通过许可赚钱），所以公司最好禁止任何“自制API”通过独立爬取其站点来收集信息。

虽然一个文件告诉你的爬虫不能去哪里起初可能看起来有些限制，但对于网络爬虫开发来说，它可能是一个隐藏的福音。如果你发现一个robots.txt文件禁止爬取网站的某个部分，这基本上是网站管理员在说，他们对爬虫在网站的其他部分是没有问题的。毕竟，如果他们对其有问题，他们在编写robots.txt时就会限制访问。

例如，适用于一般网络爬虫（与搜索引擎相对）的Wikipedia的robots.txt文件部分非常宽松。它甚至包含了欢迎机器人的人类可读文本（这就是我们！），并且只阻止了少数页面的访问，例如登录页面、搜索页面和“随机文章”页面：

```
#
# Friendly, low-speed bots are welcome viewing article pages, but not
# dynamically generated pages please.
#
# Inktomi's "Slurp" can read a minimum delay between hits; if your bot supports
# such a thing using the 'Crawl-delay' or another instruction, please let us
# know.
#
# There is a special exception for API mobileview to allow dynamic mobile web &
# app views to load section content.
# These views aren't HTTP-cached but use parser cache aggressively and don't
# expose special: pages etc.
#
User-agent: *
Allow: /w/api.php?action=mobileview&
Disallow: /w/
Disallow: /trap/
Disallow: /wiki/Especial:Search
Disallow: /wiki/Especial%3ASearch
Disallow: /wiki/Special:Collection
Disallow: /wiki/Spezial:Sammlung
Disallow: /wiki/Special:Random
Disallow: /wiki/Special%3ARandom
Disallow: /wiki/Special:Search
Disallow: /wiki/Special%3ASearch
Disallow: /wiki/Spesial:Search
Disallow: /wiki/Spesial%3ASearch
Disallow: /wiki/Spezial:Search
Disallow: /wiki/Spezial%3ASearch
Disallow: /wiki/Specjalna:Search
Disallow: /wiki/Specjalna%3ASearch
Disallow: /wiki/Speciaal:Search
Disallow: /wiki/Speciaal%3ASearch
Disallow: /wiki/Speciaal:Random
Disallow: /wiki/Speciaal%3ARandom
Disallow: /wiki/Speciel:Search
Disallow: /wiki/Speciel%3ASearch
Disallow: /wiki/Speciale:Search
Disallow: /wiki/Speciale%3ASearch
Disallow: /wiki/Istimewa:Search
Disallow: /wiki/Istimewa%3ASearch
Disallow: /wiki/Toiminnot:Search
Disallow: /wiki/Toiminnot%3ASearch
```

是否选择编写遵守robots.txt的网络爬虫取决于你，但我强烈推荐这样做，特别是如果你有无差别爬取网络的爬虫。



## 三个网络爬虫案例

由于网络爬虫的应用范围如此广泛，有很多方式可以使你陷入法律困境。本节介绍了三个涉及某种通常适用于网络爬虫的法律的案例，以及这些法律在具体案例中的应用。

### eBay诉Bidder's Edge及对动产的侵害

1997年，Beanie Baby市场火爆，科技行业蓬勃发展，在线拍卖行成为互联网的新宠。一家公司叫Bidder's Edge成立，创建了一种新型的元拍卖网站。它不再强迫你在不同的拍卖网站之间来回比较价格，而是聚合所有当前拍卖的特定产品（比如，一款热门的Furby娃娃或一张《香料世界》CD）的数据，并指向价格最低的网站。

Bidder's Edge通过一支网络爬虫大军实现了这一点，这些爬虫不断向各拍卖网站的服务器发出请求以获取价格和产品信息。在所有拍卖网站中，eBay是最大的，而Bidder's Edge每天向eBay的服务器发送约10万次请求。即使按今天的标准，这也是很大的流量。根据eBay的说法，这占其当时总互联网流量的1.53%，显然eBay对此并不满意。

eBay向Bidder's Edge发出停止和终止信，并提供了其数据的许可，但双方未能达成许可协议，Bidder's Edge继续爬取eBay的网站。

eBay尝试阻止Bidder's Edge使用的IP地址，封锁了169个IP地址，但Bidder's Edge通过使用代理服务器（代表其他机器转发请求的服务器，但使用代理服务器自己的IP地址）绕过了这些封锁。可以想象，这对双方来说都是一个令人沮丧且不可持续的解决方案——Bidder's Edge不断寻找新的代理服务器并购买新的IP地址，而旧的IP地址被封锁，eBay则不得不维护庞大的防火墙列表（并在每次数据包检查中增加计算量大的IP地址比较开销）。

最终，在1999年12月，eBay以动产侵害的罪名起诉Bidder's Edge。由于eBay的服务器是其拥有的真实、具体的资源，而它不欣赏Bidder's Edge对其的异常使用，动产侵害似乎是理想的法律依据。事实上，在现代，动产侵害与网络爬虫诉讼密不可分，通常被认为是一种信息技术法律。

法院裁定，要让eBay通过动产侵害赢得诉讼，eBay必须证明两点：

* Bidder's Edge知道其明确被禁止使用eBay的资源。
* eBay因Bidder's Edge的行为遭受了经济损失。

鉴于eBay的停止和终止信的记录，加上IT记录显示的服务器使用情况和与服务器相关的实际成本，eBay相对容易证明这一点。当然，没有任何大型法庭案件会轻易结束：双方提起了反诉，许多律师得到了报酬，最终在2001年3月案件在庭外和解，金额未披露。

那么，这是否意味着任何未经授权使用他人服务器的行为都会自动构成动产侵害的违法行为？不一定。Bidder's Edge是一个极端案例；它使用了eBay如此多的资源，以至于公司不得不购买额外的服务器，支付更多的电费，甚至可能雇佣额外的人员。虽然1.53%的增加看起来不多，但在大公司中，这可能会累计成一笔可观的费用。

2003年，加利福尼亚州最高法院裁定了另一起案件，即Intel公司诉Hamidi案。在该案中，一名前Intel员工（Hamidi）向Intel员工发送了Intel不喜欢的电子邮件，邮件通过Intel的服务器传送。法院表示：

>“Intel的主张失败了，不是因为通过互联网传输的电子邮件享有独特的豁免权，而是因为在加利福尼亚州，动产侵害罪——与前述的诉因不同——不能在没有证据表明对原告个人财产或其法律利益造成损害的情况下成立。”

基本上，Intel未能证明Hamidi向所有员工发送的六封电子邮件的传输成本（有趣的是，每封邮件都包含从Hamidi的邮件列表中删除的选项——至少他还是很有礼貌的！）对Intel造成了任何经济损失。它没有剥夺Intel任何财产或其使用权。



### 美国诉奥恩海默案与《计算机欺诈和滥用法》

如果某信息通过网页浏览器可以轻松获取，那么以自动化方式获取相同信息不太可能让你陷入联邦调查局的困境。然而，对一个足够好奇的人来说，发现一个小的安全漏洞很容易，但当自动化爬虫介入时，这个小漏洞可能很快变成一个更大、更危险的问题。

2010年，安德鲁·奥恩海默（Andrew Auernheimer）和丹尼尔·斯皮特勒（Daniel Spitler）注意到了iPad的一个有趣功能：当你使用iPad访问AT&T的网站时，AT&T会将你重定向到一个包含你iPad唯一ID号的URL：

```
https://dcp2.att.com/OEPClient/openPage?ICCID=<idNumber>&IMEI=
```

该页面包含一个登录表单，表单上有与URL中ID号对应的用户的电子邮件地址。用户只需输入密码即可访问自己的账户。

尽管可能的iPad ID号数量庞大，但通过一个网络爬虫，还是可以遍历这些可能的ID号并收集电子邮件地址。通过提供这个便捷的登录功能，AT&T实际上将其客户的电子邮件地址公开给了网络。

奥恩海默和斯皮特勒创建了一个爬虫，收集了11.4万个电子邮件地址，其中包括名人、首席执行官和政府官员的私人电子邮件地址。奥恩海默（但不是斯皮特勒）随后将名单和获取方式发送给了Gawker媒体，后者在标题为“苹果最严重的安全漏洞：11.4万iPad用户信息泄露”的报道中披露了这一消息（但未公布名单）。

2011年6月，联邦调查局因收集电子邮件地址而搜查了奥恩海默的家，虽然他们最终因毒品指控逮捕了他。2012年11月，他被判犯有身份欺诈和共谋未经授权访问计算机罪，随后被判处41个月联邦监禁，并被命令支付7.3万美元的赔偿。

他的案件引起了民权律师奥林·克尔（Orin Kerr）的注意，他加入了奥恩海默的法律团队，并向第三巡回上诉法院提出上诉。2014年4月11日（法律程序可能需要相当长的时间），他们提出了这样的论点：

奥恩海默在第一项指控上的定罪必须推翻，因为访问一个公开可用的网站不构成《计算机欺诈和滥用法》（18 U.S.C. § 1030(a)(2)(C)）下的未经授权访问。AT&T选择不使用密码或任何其他保护措施来控制对其客户电子邮件地址的访问。AT&T主观上希望外人不要发现这些数据，或者奥恩海默夸张地将访问描述为“盗窃”是无关紧要的。公司配置其服务器以使信息对所有人可见，从而授权公众查看这些信息。通过AT&T的公共网站访问电子邮件地址在CFAA下是被授权的，因此不是犯罪。

虽然奥恩海默的定罪因场所不足而在上诉中被推翻，但第三巡回法院在其裁决的脚注中似乎对这一论点表示支持：

虽然我们不需要解决奥恩海默的行为是否涉及此类侵入，但审判中没有证据表明账户爬虫曾经突破任何密码门或其他基于代码的屏障。账户爬虫只是访问了登录屏幕的公共部分，并抓取了AT&T无意中发布的信息。

虽然奥恩海默最终没有因《计算机欺诈和滥用法》而被定罪，但他的家被联邦调查局搜查，他花费了数千美元的法律费用，并在法庭和监狱中度过了三年。

作为网络爬虫使用者，我们可以从中汲取哪些教训以避免类似情况？或许一个好的开始是：不要做混蛋。

爬取任何形式的敏感信息，无论是个人数据（在本案中是电子邮件地址）、商业机密或政府机密，可能都是你不想在没有律师随时待命的情况下做的事情。即使是公开可用的信息，想一想：“普通计算机用户是否能够轻松访问这些信息，如果他们想查看的话？”或“这是公司希望用户看到的内容吗？”

我多次致电公司报告其网络应用程序中的安全漏洞。这个方式效果显著：“你好，我是一名安全专家，发现了你们网站上的一个潜在漏洞。能否引导我找到相关人员报告并解决这个问题？”除了立即得到对你（白帽）黑客天才的认可外，你可能还会获得免费的订阅、现金奖励和其他好处！

此外，奥恩海默将信息发布给Gawker媒体（而不是先通知AT&T）以及他对漏洞利用的炫耀行为，也使他成为AT&T律师的特别目标。

如果你在某个网站上发现安全漏洞，最好的做法是通知该网站的所有者，而不是媒体。你可能会想写一篇博客文章并向全世界宣布，特别是如果问题没有立即得到解决的话。然而，你需要记住，这是公司的责任，而不是你的。你能做的最好的事情就是把你的网络爬虫（如果适用的话，还有你的业务）从该网站上撤走！


### 菲尔德 vs Google: 版权和robots.txt



布莱克·菲尔德（Blake Field）是一名律师，他以谷歌的网站缓存功能违反版权法为由提起了诉讼，因为在他从自己的网站上移除书籍后，谷歌显示了该书的副本。版权法允许原创作品的创作者对该作品的分发拥有控制权。菲尔德的论点是，谷歌的缓存（在他从网站上移除后）剥夺了他控制其分发的能力。

**谷歌网页缓存**

当谷歌网络爬虫（也称为Googlebots）爬取网站时，它们会复制该网站并将其托管在互联网上。任何人都可以通过以下URL格式访问此缓存：http://webcache.googleusercontent.com/search?q=cache:http://pythonscraping.com
如果你正在搜索或爬取的网站不可用，你可以在那里查看是否存在可用的副本！

了解谷歌的缓存功能但未采取行动并没有帮助菲尔德的案件。毕竟，他本可以通过添加robots.txt文件简单指示哪些页面应该被爬取，哪些不应该被爬取来阻止谷歌爬虫缓存他的网站。

更重要的是，法院发现《数字千年版权法》（DMCA）的安全港规定允许谷歌合法地缓存和显示菲尔德等网站：“服务提供商不应因在由其控制或为其服务的系统或网络上中间和临时存储材料而导致的版权侵权而被要求支付金钱赔偿。”



 




