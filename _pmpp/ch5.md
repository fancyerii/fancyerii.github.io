---
layout:     post
title:      "第五章：内存架构和数据局部性"
author:     "lili"
mathjax: true
sticky: true
excerpt_separator: <!--more-->
tags:
    - gpu
    - cuda 
---



 <!--more-->


到目前为止，我们已经学习了如何编写一个CUDA核函数，并如何配置和协调其执行，以供大量线程使用。我们还研究了当前GPU硬件的计算架构以及如何在该硬件上调度线程执行。在本章中，我们将专注于GPU的片上内存架构，并开始研究如何组织和定位数据，以便大量线程高效访问。到目前为止，我们研究过的CUDA核函数可能只能实现底层硬件潜在速度的一小部分。这种性能不佳是因为全局内存通常使用芯片外DRAM实现，具有较长的访问延迟（数百个时钟周期）和有限的访问带宽。虽然理论上拥有许多可用于执行的线程可以容忍长时间的内存访问延迟，但很容易遇到这样的情况：全局内存访问路径中的交通拥堵阻止了除了极少数线程之外的所有线程都取得进展，从而使一些流多处理器（SMs）中的核心处于空闲状态。为了避免这种拥堵，GPU提供了许多额外的片上内存资源，用于访问数据，可以消除大部分与全局内存的通信。在本章中，我们将研究不同类型的内存用于提升CUDA核函数的执行性能。




 
## 5.1 内存访问效率的重要性

我们可以通过计算图3.11中最常执行的矩阵乘法核心代码的预期性能水平来说明内存访问效率的影响，该代码在图5.1中部分复制。就执行时间而言，核心的最重要部分是执行 M 行与 N 列的点积的 for 循环。

<a>![](/img/pmpp/ch5/1.png)</a>
*图5.1 矩阵乘法核心的最常执行部分。*

在循环的每次迭代中，为了一个浮点乘法和一个浮点加法，会执行两次全局内存访问。全局内存访问会从 M 和 N 数组中获取元素。浮点乘法操作将这两个元素相乘，浮点加法操作将乘积累加到 Pvalue 中。因此，浮点操作（FLOP）与从全局内存访问的字节数（B）的比率为2 FLOP 对 8 B，即 0.25 FLOP/B。我们将这个比率称为计算与全局内存访问的比率，它定义为程序区域内每个字节访问的全局内存所执行的 FLOP 数。在文献中，这个比率有时也称为算术强度或计算强度。

计算与全局内存访问的比率对CUDA核函数的性能有重大影响。例如，Ampere A100 GPU 的峰值全局内存带宽为 1555 GB/秒。由于矩阵乘法核心执行的操作是 0.25 OP/B，因此全局内存带宽限制了内核可以执行的单精度FLOP的吞吐量，为每秒 389 giga FLOP（GFLOPS），通过将 1555 GB/秒 与 0.25 FLOP/B 相乘获得。然而，389 GFLOPS仅占A100 GPU的峰值单精度操作吞吐量的2%，后者为 19,500 GFLOPS。A100还配备了称为张量核心的专用单元，用于加速矩阵乘法运算。如果考虑到A100的张量核心峰值单精度浮点吞吐量为156,000 GFLOPS，那么389 GFLOPS仅占峰值的0.25%。因此，矩阵乘法核心的执行受限于数据从内存传输到GPU核心的速率。我们将受内存带宽限制的程序的执行速度称为内存密集型(bound)程序。

**屋顶线(Roofline)模型**

屋顶线模型是一种用于评估应用程序相对于其运行硬件极限所实现性能的视觉模型。下面是屋顶线模型的一个基本示例。

<a>![](/img/pmpp/ch5/2.png)</a>


在x轴上，我们有以FLOP/B为单位的算术或计算强度。它反映了应用程序为每个加载的字节数据所执行的工作量。在y轴上，我们有以GFLOPS为单位的计算吞吐量。图中的两条线反映了硬件的限制。水平线由硬件可以维持的峰值计算吞吐量（GFLOPS）确定。从原点开始的带有正斜率的线由硬件可以维持的峰值内存带宽确定。图中的一个点代表了一个应用程序，其操作强度在x轴上，其在y轴上实现的计算吞吐量。当然，这些点将位于两条线的下方，因为它们不能实现比硬件峰值更高的吞吐量。

点相对于这两条线的位置告诉我们有关应用程序效率的信息。靠近这两条线的点表明应用程序有效地使用了内存带宽或计算单元，而远离这两条线的应用程序表示资源使用不足。这两条线的交点表示应用程序从内存密集型转换为计算密集型的计算强度值。计算强度较低的应用程序是内存密集型的，并且无法实现峰值吞吐量，因为它们受到内存带宽的限制。计算强度较高的应用程序是计算密集型的，并且不受内存带宽的限制。

举例来说，点 A1 和 A2 都代表内存密集型应用程序，而 A3 代表计算密集型应用程序。A1有效地利用资源，并且接近峰值内存带宽，而 A2 则没有。对于A2，可能有进一步优化的空间，以通过改善内存带宽利用率来提高吞吐量。然而，对于A1，提高吞吐量的唯一方法是增加应用程序的计算强度。

要提高此核心的性能，我们需要通过减少它执行的全局内存访问次数来提高核心的计算与全局内存访问比率。例如，为了充分利用A100 GPU提供的 19,500 GFLOPS，至少需要（19,500 GOP/秒）/（1555 GB/秒）=12.5 OP/B的比率。这个比率意味着，对于每个访问的 4 字节浮点值，必须执行大约 50 次浮点操作！这种比率能够达到的程度取决于正在进行的计算中固有的数据重用。我们建议读者阅读“屋顶线模型”旁注，以了解有关使用计算强度分析程序潜在性能的有用模型。

正如我们将看到的，矩阵乘法提供了减少全局内存访问的机会，这可以通过相对简单的技术来捕获。矩阵乘法函数的执行速度可以根据全局内存访问的减少程度而相差几个数量级。因此，矩阵乘法为这些技术提供了一个极好的初始示例。本章介绍了一种常用的减少全局内存访问次数的技术，并在矩阵乘法上演示了该技术。



## 5.2 CUDA内存类型

CUDA设备包含几种类型的内存，可以帮助程序员改善计算与全局内存访问的比率。图5.2显示了这些CUDA设备内存。在图的底部，我们看到全局内存和常量内存。这两种类型的内存都可以由主机进行写入（W）和读取（R）。全局内存也可以由设备进行写入和读取，而常量内存则支持设备进行短延迟、高带宽的只读访问。我们在第2章异构数据并行计算中介绍了全局内存，而在第7章卷积中将详细讨论常量内存。

<a>![](/img/pmpp/ch5/3.png)</a>
**图 5.2 CUDA设备内存模型的（不完整）概述。图中未显示的一个重要类型的CUDA内存是纹理内存，因为本教材未涵盖其使用。**


另一种类型的内存是本地内存，它也可以被读取和写入。本地内存实际上位于全局内存中，并具有类似的访问延迟，但它不跨线程共享。每个线程都有自己的一部分全局内存，它将其用作自己的私有本地内存，其中放置了线程私有但无法在寄存器中分配的数据。这些数据包括静态分配的数组、溢出的寄存器和线程调用堆栈的其他元素。

图5.2中的寄存器和共享内存是芯片上的内存。驻留在这些类型的内存中的变量可以以非常高的速度以高度并行的方式访问。寄存器分配给各个线程；每个线程只能访问自己的寄存器。典型的内核函数通常使用寄存器来保存对每个线程私有的频繁访问的变量。共享内存分配给线程块；块中的所有线程都可以访问为该块声明的共享内存变量。共享内存是线程通过共享其输入数据和中间结果来合作的有效手段。通过在CUDA内存类型中声明CUDA变量，CUDA程序员决定了变量的可见性和访问速度。


**CPU与GPU寄存器架构**

CPU和GPU之间不同的设计目标导致了不同的寄存器架构。正如我们在第4章计算架构和调度中看到的那样，当CPU在不同线程之间进行上下文切换时，它将外出线程的寄存器保存到内存中，并从内存中恢复传入线程的寄存器。相比之下，GPU通过将所有计划在处理块上的线程的寄存器保留在处理块的寄存器文件中来实现零开销调度。这样，线程warp之间的切换是即时的，因为传入线程的寄存器已经在寄存器文件中。因此，GPU寄存器文件的大小需要比CPU寄存器文件大得多。

我们还在第4章计算架构和调度中看到，GPU支持动态资源分区，其中SM可以为每个线程提供少量的寄存器并执行大量的线程，或者为每个线程提供更多的寄存器并执行较少的线程。因此，GPU寄存器文件需要设计以支持寄存器的动态分区。相比之下，CPU寄存器架构为每个线程的寄存器分配了固定的寄存器集，而不考虑线程对寄存器的实际需求。


<a>![](/img/pmpp/ch5/4.png)</a>
*图 5.3 基于冯·诺依曼模型的现代计算机中的内存与寄存器。*

要充分了解寄存器、共享内存和全局内存之间的差异，我们需要更详细地了解这些不同类型的内存如何在现代处理器中实现和使用。正如我们在第4章计算架构和调度的“Warp和SIMD硬件”中讨论的那样，几乎所有现代处理器都源自1945年约翰·冯·诺依曼提出的模型，如图5.3所示。CUDA设备也不例外。CUDA设备中的全局内存映射到图5.3中的内存框。处理器框对应于我们今天通常看到的处理器芯片边界。全局内存位于处理器芯片外部，并采用DRAM技术实现，这意味着访问延迟较长，访问带宽相对较低。寄存器对应于冯·诺依曼模型中的“寄存器文件”。寄存器文件位于处理器芯片上，这意味着访问延迟非常短，访问带宽与全局内存相比大大提高。在典型的设备中，所有SM上所有寄存器文件的聚合访问带宽至少比全局内存高两个数量级。此外，当变量存储在寄存器中时，其访问不再消耗芯片外全局内存带宽。这将反映为计算与全局内存访问比率的增加。


在现代计算机中，每次访问寄存器都比访问全局内存涉及的指令更少，这是一个微妙的点。在大多数现代处理器中，算术指令都具有“内置”寄存器操作数。例如，浮点加法指令可能具有以下形式：

```
fadd r1, r2, r3
```

其中r2和r3是指定寄存器文件中可以找到输入操作数值的位置的寄存器编号。存储浮点加法结果值的位置由r1指定。因此，当算术指令的操作数在寄存器中时，不需要额外的指令将操作数值提供给算术逻辑单元（ALU），即进行算术计算的地方。

与此同时，如果操作数值在全局内存中，处理器需要执行内存加载操作将操作数值提供给ALU。例如，如果浮点加法指令的第一个操作数在全局内存中，涉及的指令可能看起来像以下示例：

```
load r2, r4, offset
fadd r1, r2, r3
```


其中load指令将偏移值添加到r4的内容中以形成操作数值的地址。然后，它访问全局内存并将该值放入寄存器r2中。一旦操作数值在r2中，fadd指令就会使用r2和r3中的值执行浮点加法，并将结果放入r1中。由于处理器每个时钟周期只能获取和执行有限数量的指令，带有额外加载的版本可能比没有额外加载的版本需要更长时间来处理。这是将操作数放入寄存器可以提高执行速度的另一个原因。

最后，将操作数值放入寄存器更可取的另一个微妙原因。在现代计算机中，从寄存器文件访问值所消耗的能量至少比从全局内存访问值的能量低一个数量级。与从全局内存访问值相比，从寄存器访问值在能量效率方面具有巨大优势。我们很快将详细了解访问这两种硬件结构中的速度和能量差异。另一方面，正如我们很快将了解的那样，每个线程可用的寄存器数量在今天的GPU中是相当有限的。正如我们在第4章计算架构和调度中看到的那样，在全占用场景中，如果寄存器使用量超过限制，应用程序的占用率可能会降低。因此，我们也需要尽可能避免超额订阅这一有限资源。

<a>![](/img/pmpp/ch5/5.png)</a>
*图 5.4 CUDA设备SM中的共享内存与寄存器的对比。*

图5.4显示了CUDA设备中的共享内存和寄存器。尽管两者都是芯片上的内存，但在功能和访问成本上有很大差异。共享内存设计为驻留在处理器芯片上的内存空间的一部分。当处理器访问驻留在共享内存中的数据时，它需要执行内存加载操作，就像访问全局内存中的数据一样。但是，由于共享内存位于芯片上，它的访问延迟和吞吐量比全局内存要低得多。由于需要执行加载操作，共享内存的延迟和带宽低于寄存器。在计算机体系结构术语中，共享内存是一种暂存器(scratchpad)内存形式。

CUDA中共享内存和寄存器之间的一个重要区别是，驻留在共享内存中的变量可以被块中的所有线程访问。这与寄存器数据相反，后者是线程私有的。也就是说，共享内存旨在支持块中的线程之间的高效、高带宽的数据共享。如图5.4所示，CUDA设备SM通常使用多个处理单元允许多个线程同时进行进度（请参阅第2章异构数据并行计算中的“线程”侧栏）。块中的线程可以分布在这些处理单元上。因此，这些CUDA设备中共享内存的硬件实现通常设计为允许多个处理单元同时访问其内容，以支持块中线程之间的高效数据共享。

到目前为止，应该清楚寄存器、本地内存、共享内存和全局内存都具有不同的功能、延迟和带宽。因此，了解如何声明变量以使其驻留在所需类型的内存中非常重要。表5.1提供了将程序变量声明为各种内存类型的CUDA语法。每个这样的声明还为其声明的CUDA变量指定了作用域和生命周期。作用域标识了可以访问变量的线程集：单个线程、块中的所有线程或所有网格的所有线程。如果变量的作用域是单个线程，则为每个线程创建变量的私有版本；每个线程只能访问其私有版本的变量。例如，如果一个内核声明的变量的作用域是一个线程，并且启动了一百万个线程，将创建一百万个版本的变量，以便每个线程初始化和使用其自己的变量版本。

<a>![](/img/pmpp/ch5/6.png)</a>
*表5.1 CUDA变量声明类型修饰符及每种类型的特性。*


生命周期告诉我们程序执行期间变量可供使用的部分：可以是在网格执行期间，也可以是整个应用程序期间。如果变量的生命周期在网格执行期间，它必须在内核函数体内声明，并且只能由内核代码使用。如果内核被多次调用，则该变量的值不会在这些调用之间保持。每次调用都必须初始化变量才能使用。另一方面，如果变量的生命周期是整个应用程序期间，它必须在任何函数体外声明。这些变量的内容在应用程序执行期间保持不变，并且可供所有内核使用。

我们将非数组的变量称为标量变量。如表 5.1 所示，所有在内核和设备函数中声明的自动标量变量都被放置到寄存器中。这些自动变量的作用域限定在单个线程内。当内核函数声明一个自动变量时，对于执行内核函数的每个线程，都会生成该变量的私有副本。当线程终止时，所有其自动变量都将不复存在。在图 5.1 中，变量 blurRow、blurCol、curRow、curCol、pixels 和 pixVal 都是自动变量，属于这个类别。请注意，访问这些变量非常快速和并行化，但必须小心不要超出硬件实现中寄存器存储的有限容量。使用大量寄存器可能会对每个 SM 的占用产生负面影响，正如我们在第 4 章，计算架构和调度中看到的那样。

自动数组变量不存储在寄存器中(这个规则也有一些例外情况。如果所有访问都是使用常量索引值完成的，编译器可能会决定将自动数组存储到寄存器中。)。相反，它们存储在线程的本地内存中，并可能产生长时间的访问延迟和潜在的访问拥塞。这些数组的作用域，就像自动标量变量一样，限定在单个线程内。也就是说，为每个线程创建并由每个线程使用自动数组的私有版本。一旦线程终止其执行，其自动数组变量的内容就会消失。根据我们的经验，很少需要在内核函数和设备函数中使用自动数组变量。

如果变量声明前带有 \_\_shared\_\_ 关键字（每个“\_\_”由两个“\_”字符组成），则它在 CUDA 中声明了一个共享变量。也可以在声明前面添加可选的 \_\_device\_\_ 来达到相同的效果。这样的声明通常在内核函数或设备函数中进行。共享变量驻留在共享内存中。共享变量的作用域在一个线程块内；也就是说，一个块内的所有线程看到同一个共享变量的版本。在内核执行期间，为每个块创建并使用共享变量的私有版本。共享变量的生命周期在内核执行期间。当内核终止其网格的执行时，共享变量的内容也将不复存在。正如我们之前讨论过的，共享变量是块内线程之间协作的有效手段。从共享内存中访问共享变量非常快速和高度并行化。CUDA 程序员通常使用共享变量来保存在内核执行阶段中经常使用和重复使用的全局内存数据的部分。可能需要调整用于创建执行阶段的算法，以便重点关注全局内存数据的小部分，就像我们将在第 5.4 节中通过矩阵乘法演示的那样。

如果变量声明前带有关键字 \_\_constant（每个“\_\_”由两个“\_”字符组成），则它在 CUDA 中声明了一个常量变量。也可以在声明前面添加可选的 \_\_device\_\_ 来达到相同的效果。常量变量的声明必须在任何函数体外。常量变量的作用域是所有网格，这意味着所有网格中的所有线程看到同一个常量变量的版本。常量变量的生命周期是整个应用程序的执行期间。常量变量通常用于向内核函数提供输入值的变量。常量变量的值不能被内核函数代码更改。常量变量存储在全局内存中，但会进行缓存以实现高效访问。通过适当的访问模式，访问常量内存是非常快速和并行化的。目前，应用程序中常量变量的总大小限制为 65,536 字节。可能需要分割输入数据量以适应此限制。我们将在第 7 章，卷积中演示常量内存的使用。

如果变量声明仅由关键字 \_\_device\_\_（每个“\_\_”由两个“\_”字符组成）前置，那么它是一个全局变量，并将放置在全局内存中。访问全局变量的速度较慢。最近的设备中使用缓存提高了访问全局变量的延迟和吞吐量。全局变量的一个重要优势是它们对所有内核的所有线程可见。它们的内容也在整个执行期间持续存在。因此，全局变量可以用作跨块协作的手段。然而，必须注意，目前没有简单的方法可以在不使用原子操作或终止当前内核执行的情况下，在来自不同线程块的线程之间同步，或确保全局内存中的数据一致性。因此，全局变量通常用于将信息从一个内核调用传递到另一个内核调用。

在 CUDA 中，指针可以用于指向全局内存中的数据对象。指针在内核和设备函数中使用有两种典型方式。首先，如果一个对象是由主机函数分配的，那么该对象的指针将由内存分配 API 函数（如 cudaMalloc）初始化，并且可以作为参数传递给内核函数，正如我们在第 2 章，异构数据并行计算，和第 3 章，多维网格和数据中看到的那样。第二种使用方式是将在全局内存中声明的变量的地址分配给指针变量。例如，在内核函数中的语句 {float* ptr=&GlobalVar;} 将 GlobalVar 的地址分配给自动指针变量 ptr。读者应参考 CUDA 编程指南了解在其他内存类型中使用指针的方法。


